{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Copyright 2023 Google LLC. Double-click for license information.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "kYmgbnGytC9h",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeNGK50ZP5pR"
      },
      "source": [
        "# Colabs\n",
        "\n",
        "Machine Learning Crash Course uses Colaboratories (Colabs) for all programming exercises. Colab is Google's implementation of [Jupyter Notebook](https://jupyter.org/). For more information about Colabs and how to use them, go to [Welcome to Colaboratory](https://research.google.com/colaboratory).\n",
        "\n",
        "# Binary classification\n",
        "\n",
       "In this Colab, you'll complete the following tasks:\n",
       "- Examine a dataset containing measurements derived from images of two species of Turkish rice.\n",
       "- Create a binary classifier to sort grains of rice into the two species.\n",
       "- Evaluate the performance of the model.\n",
       "\n", 
        "## Learning objectives\n",
        "\n",
        "By completing this Colab, you'll learn:\n",
        "- How to train a binary classifier.\n",
        "- How to calculate metrics for a binary classifier at different thresholds.\n",
        "- How to compare AUC and ROC of two different models.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "This Colab uses the Cinar and Koklu 2019 Osmancik and Cammeo rice dataset.\n",
        "\n",
        "Provided with a CC0 license (see [Kaggle](https://www.kaggle.com/datasets/muratkokludataset/rice-dataset-commeo-and-osmancik) for more documentation; lengths and area are given in pixels). Cinar and Koklu also provide datasets for multiclass (5 species of rice), pistachios, raisins, grape leaves, and so on, at their [repository](https://www.muratkoklu.com/datasets/).\n",
        "\n",
        "### Citation\n",
        "\n",
        "Cinar, I. and Koklu, M., (2019). “Classification of Rice Varieties Using Artificial Intelligence Methods.” *International Journal of Intelligent Systems and Applications in Engineering*, 7(3), 188-194.\n",
        "\n",
        "DOI: https://doi.org/10.18201/ijisae.2019355381\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4JwY1X5iryL"
      },
      "source": [
        "# Load Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "irD_75BI97__"
      },
      "outputs": [],
      "source": [
        "# @title Load the imports\n",
        "\n",
        "import io\n",
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "print(\"Ran the import statements.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aDobhxERWPD1"
      },
      "outputs": [],
      "source": [
        "# @title Load the dataset\n",
        "rice_dataset_raw = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/Rice_Cammeo_Osmancik.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IqvqOvaQqlK"
      },
      "source": [
        "Once the dataset has been loaded via the cell above, select specific columns to show summary statistics of the numerical features in the dataset.\n",
        "\n",
        "See the Kaggle [dataset documentation](https://www.kaggle.com/datasets/muratkokludataset/rice-dataset-commeo-and-osmancik), especially the **Provenance** section, for explanations of what each feature means and how they were calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XKakOMCmHp-E"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Read and provide statistics on the dataset.\n",
        "rice_dataset = rice_dataset_raw[[\n",
        "    'Area',\n",
        "    'Perimeter',\n",
        "    'Major_Axis_Length',\n",
        "    'Minor_Axis_Length',\n",
        "    'Eccentricity',\n",
        "    'Convex_Area',\n",
        "    'Extent',\n",
        "    'Class',\n",
        "]]\n",
        "\n",
        "rice_dataset.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynv9WwTPG9oU"
      },
      "source": [
        "## Task 1: Describe the data\n",
        "\n",
        "From the summary statistics above, answer the following questions:\n",
        "- What are the min and max lengths (major axis length, given in pixels) of the rice grains?\n",
        "- What is the range of areas between the smallest and largest rice grains?\n",
        "- How many standard deviations (`std`) is the largest rice grain's perimeter from the mean?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y36kQm1vJ7n3"
      },
      "outputs": [],
      "source": [
        "# @title Solutions (run the cell to get the answers)\n",
        "\n",
        "print(\n",
        "    f'The shortest grain is {rice_dataset.Major_Axis_Length.min():.1f}px long,'\n",
        "    f' while the longest is {rice_dataset.Major_Axis_Length.max():.1f}px.'\n",
        ")\n",
        "print(\n",
        "    f'The smallest rice grain has an area of {rice_dataset.Area.min()}px, while'\n",
        "    f' the largest has an area of {rice_dataset.Area.max()}px.'\n",
        ")\n",
        "print(\n",
        "    'The largest rice grain, with a perimeter of'\n",
        "    f' {rice_dataset.Perimeter.max():.1f}px, is'\n",
        "    f' ~{(rice_dataset.Perimeter.max() - rice_dataset.Perimeter.mean())/rice_dataset.Perimeter.std():.1f} standard'\n",
        "    f' deviations ({rice_dataset.Perimeter.std():.1f}) from the mean'\n",
        "    f' ({rice_dataset.Perimeter.mean():.1f}px).'\n",
        ")\n",
        "print(\n",
        "    f'This is calculated as: ({rice_dataset.Perimeter.max():.1f} -'\n",
        "    f' {rice_dataset.Perimeter.mean():.1f})/{rice_dataset.Perimeter.std():.1f} ='\n",
        "    f' {(rice_dataset.Perimeter.max() - rice_dataset.Perimeter.mean())/rice_dataset.Perimeter.std():.1f}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc8JVgN2j6h3"
      },
      "source": [
        "# Explore the dataset\n",
        "\n",
        "Plot some of the features against each other, including in 3D.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNw5U7-4NFLR"
      },
      "outputs": [],
      "source": [
        "# Create five 2D plots of the features against each other, color-coded by class.\n",
        "for x_axis_data, y_axis_data in [\n",
        "    ('Area', 'Eccentricity'),\n",
        "    ('Convex_Area', 'Perimeter'),\n",
        "    ('Major_Axis_Length', 'Minor_Axis_Length'),\n",
        "    ('Perimeter', 'Extent'),\n",
        "    ('Eccentricity', 'Major_Axis_Length'),\n",
        "]:\n",
        "  px.scatter(rice_dataset, x=x_axis_data, y=y_axis_data, color='Class').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6xJ0HQxLB4N"
      },
      "source": [
        "## Task 2: Visualize samples in 3D\n",
        "\n",
        "Try graphing three of the features in 3D against each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qvpUsZF1LDWM"
      },
      "outputs": [],
      "source": [
        "#@title Plot three features in 3D by entering their names and running this cell\n",
        "\n",
        "x_axis_data = 'Enter a feature name here'  # @param {type: \"string\"}\n",
        "y_axis_data = 'Enter a feature name here'  # @param {type: \"string\"}\n",
        "z_axis_data = 'Enter a feature name here'  # @param {type: \"string\"}\n",
        "\n",
        "px.scatter_3d(\n",
        "    rice_dataset,\n",
        "    x=x_axis_data,\n",
        "    y=y_axis_data,\n",
        "    z=z_axis_data,\n",
        "    color='Class',\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r5WBGiJChXt-"
      },
      "outputs": [],
      "source": [
        "# @title One possible solution\n",
        "\n",
        "# Plot major and minor axis length and eccentricity, with observations\n",
        "# color-coded by class.\n",
        "px.scatter_3d(\n",
        "    rice_dataset,\n",
        "    x='Eccentricity',\n",
        "    y='Area',\n",
        "    z='Major_Axis_Length',\n",
        "    color='Class',\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch82395CSBMR"
      },
      "source": [
        "If we were to pick three features, it seems that major axis length, area, and eccentricity might contain most of the information that differentiates the two classes. Other combinations may work as well.\n",
        "\n",
        "Run the previous code cell to graph those three features if you haven't already.\n",
        "\n",
        "It seems like a distinct class boundary appears in the plane of these three features. We'll train a model on just these features, then another model on the complete set of features, and compare their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G6y-XcEmk6r"
      },
      "source": [
        "## Normalize data\n",
        "\n",
        "When creating a model with multiple features, the values of each feature should span roughly the same range.  If one feature's values range from 500 to 100,000 and another feature's values range from 2 to 12, the model will need to have weights of extremely low or extremely high values to be able to combine these features effectively. This could result in a low quality model.  To avoid this,\n",
        "[normalize](https://developers.google.com/machine-learning/glossary/#normalization) features in a multi-feature model.\n",
        "\n",
        "This can be done by converting each raw value to its Z-score. The **Z-score** for a given value is how many standard deviations away from the mean the value is.\n",
        "\n",
        "Consider a feature with a mean of 60 and a standard deviation of 10.\n",
        "\n",
        "The raw value 75 would have a Z-score of +1.5:\n",
        "\n",
        "```\n",
        "  Z-score = (75 - 60) / 10 = +1.5\n",
        "```\n",
        "\n",
        "The raw value 38 would have a Z-score of -2.2:\n",
        "\n",
        "```\n",
        "  Z-score = (38 - 60) / 10 = -2.2\n",
        "```\n",
        "\n",
        "Now normalize the numerical values in the rice dataset by converting them to Z-scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSUjPSwNiyBP"
      },
      "outputs": [],
      "source": [
        "# Calculate the Z-scores of each numerical column in the raw data and write\n",
        "# them into a new DataFrame named df_norm.\n",
        "\n",
        "feature_mean = rice_dataset.mean(numeric_only=True)\n",
        "feature_std = rice_dataset.std(numeric_only=True)\n",
        "numerical_features = rice_dataset.select_dtypes('number').columns\n",
        "normalized_dataset = (\n",
        "    rice_dataset[numerical_features] - feature_mean\n",
        ") / feature_std\n",
        "\n",
        "# Copy the class to the new dataframe\n",
        "normalized_dataset['Class'] = rice_dataset['Class']\n",
        "\n",
        "# Examine some of the values of the normalized training set. Notice that most\n",
        "# Z-scores fall between -2 and +2.\n",
        "normalized_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5aXjXq-YIkL"
      },
      "source": [
        "# Set the random seeds\n",
        "\n",
        "To make experiments reproducible, we set the seed of the random number generators. This means that the order in which the data is shuffled, the values of the random weight initializations, etc, will all be the same each time the colab is run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu257GAFYH-N"
      },
      "outputs": [],
      "source": [
        "keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7M9I-ekT1dV"
      },
      "source": [
        "## Label and split data\n",
        "\n",
        "To train the model, we'll arbritrarily assign the Cammeo species a label of '1' and the Osmancik species a label of '0'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4_yTxWdvPqz"
      },
      "outputs": [],
      "source": [
        "# Create a column setting the Cammeo label to '1' and the Osmancik label to '0'\n",
        "# then show 10 randomly selected rows.\n",
        "normalized_dataset['Class_Bool'] = (\n",
        "    # Returns true if class is Cammeo, and false if class is Osmancik\n",
        "    normalized_dataset['Class'] == 'Cammeo'\n",
        ").astype(int)\n",
        "normalized_dataset.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBY8b0akUqiQ"
      },
      "source": [
        "We can then randomize and partition the dataset into train, test, and validation splits, consisting of 80%, 10%, and 10% of the dataset respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XE-RAq0av1wv"
      },
      "outputs": [],
      "source": [
        "# Create indices at the 80th and 90th percentiles\n",
        "number_samples = len(normalized_dataset)\n",
        "index_80th = round(number_samples * 0.8)\n",
        "index_90th = index_80th + round(number_samples * 0.1)\n",
        "\n",
        "# Randomize order and split into train, validation, and test with a .8, .1, .1 split\n",
        "shuffled_dataset = normalized_dataset.sample(frac=1, random_state=100)\n",
        "train_data = shuffled_dataset.iloc[0:index_80th]\n",
        "validation_data = shuffled_dataset.iloc[index_80th:index_90th]\n",
        "test_data = shuffled_dataset.iloc[index_90th:]\n",
        "\n",
        "# Show the first five rows of the last split\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Iq_haqJYeSH"
      },
      "source": [
        "It's important to prevent the model from getting the label as input during training, which is called label leakage. This can be done by storing features and labels as separate variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gi0VaAAYiaO"
      },
      "outputs": [],
      "source": [
        "label_columns = ['Class', 'Class_Bool']\n",
        "\n",
        "train_features = train_data.drop(columns=label_columns)\n",
        "train_labels = train_data['Class_Bool'].to_numpy()\n",
        "validation_features = validation_data.drop(columns=label_columns)\n",
        "validation_labels = validation_data['Class_Bool'].to_numpy()\n",
        "test_features = test_data.drop(columns=label_columns)\n",
        "test_labels = test_data['Class_Bool'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-kTF2rTY-K8"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Choose the input features\n",
        "\n",
        "To start with, we'll train a model on `Eccentricity`, `Major_Axis_Length,` and `Area`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_UNIPBtjoz"
      },
      "outputs": [],
      "source": [
        "# Name of the features we'll train our model on.\n",
        "input_features = [\n",
        "    'Eccentricity',\n",
        "    'Major_Axis_Length',\n",
        "    'Area',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSBegHR5rSEn"
      },
      "source": [
        "## Define functions that build and train a model\n",
        "\n",
        "The following code cell defines two functions:\n",
        "\n",
        "  * `create_model(inputs, learning_rate, metrics)`, which defines the model's architecture.\n",
        "  * `train_model(model, dataset, epochs, label_name, batch_size, shuffle)`, uses input features and labels to train the model.\n",
        "\n",
        "Note: create_model applies the sigmoid function to perform [logistic regression](https://developers.google.com/machine-learning/crash-course/logistic-regression).\n",
        "\n",
        "We also define two helpful data structures: `ExperimentSettings` and `Experiment`. We use these simple classes to keep track of our experiments, allowing us to know what hyperparameters were used and what the results were. In `ExperimentSettings`, we store all values describing an experiment (i.e., hyperparameters). Then, we store the results of a training run (i.e., the model and the training metrics) into an `Experiment` instance, along with the `ExperimentSettings` used for that experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8B2VArcKH6UX"
      },
      "outputs": [],
      "source": [
        "# @title Define the functions that create and train a model.\n",
        "\n",
        "import dataclasses\n",
        "\n",
        "\n",
        "@dataclasses.dataclass()\n",
        "class ExperimentSettings:\n",
        "  \"\"\"Lists the hyperparameters and input features used to train am model.\"\"\"\n",
        "\n",
        "  learning_rate: float\n",
        "  number_epochs: int\n",
        "  batch_size: int\n",
        "  classification_threshold: float\n",
        "  input_features: list[str]\n",
        "\n",
        "\n",
        "@dataclasses.dataclass()\n",
        "class Experiment:\n",
        "  \"\"\"Stores the settings used for a training run and the resulting model.\"\"\"\n",
        "\n",
        "  name: str\n",
        "  settings: ExperimentSettings\n",
        "  model: keras.Model\n",
        "  epochs: np.ndarray\n",
        "  metrics_history: keras.callbacks.History\n",
        "\n",
        "  def get_final_metric_value(self, metric_name: str) -> float:\n",
        "    \"\"\"Gets the final value of the given metric for this experiment.\"\"\"\n",
        "    if metric_name not in self.metrics_history:\n",
        "      raise ValueError(\n",
        "          f'Unknown metric {metric_name}: available metrics are'\n",
        "          f' {list(self.metrics_history.columns)}'\n",
        "      )\n",
        "    return self.metrics_history[metric_name].iloc[-1]\n",
        "\n",
        "\n",
        "def create_model(\n",
        "    settings: ExperimentSettings,\n",
        "    metrics: list[keras.metrics.Metric],\n",
        ") -> keras.Model:\n",
        "  \"\"\"Create and compile a simple classification model.\"\"\"\n",
        "  model_inputs = [\n",
        "      keras.Input(name=feature, shape=(1,))\n",
        "      for feature in settings.input_features\n",
        "  ]\n",
        "  # Use a Concatenate layer to assemble the different inputs into a single\n",
        "  # tensor which will be given as input to the Dense layer.\n",
        "  # For example: [input_1[0][0], input_2[0][0]]\n",
        "\n",
        "  concatenated_inputs = keras.layers.Concatenate()(model_inputs)\n",
        "  dense = keras.layers.Dense(\n",
        "      units=1, input_shape=(1,), name='dense_layer', activation=keras.activations.sigmoid\n",
        "  )\n",
        "  model_output = dense(concatenated_inputs)\n",
        "  model = keras.Model(inputs=model_inputs, outputs=model_output)\n",
        "  # Call the compile method to transform the layers into a model that\n",
        "  # Keras can execute.  Notice that we're using a different loss\n",
        "  # function for classification than for regression.\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.RMSprop(\n",
        "          settings.learning_rate\n",
        "      ),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics,\n",
        "  )\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    experiment_name: str,\n",
        "    model: keras.Model,\n",
        "    dataset: pd.DataFrame,\n",
        "    labels: np.ndarray,\n",
        "    settings: ExperimentSettings,\n",
        ") -> Experiment:\n",
        "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
        "\n",
        "  # The x parameter of keras.Model.fit can be a list of arrays, where\n",
        "  # each array contains the data for one feature.\n",
        "  features = {\n",
        "      feature_name: np.array(dataset[feature_name])\n",
        "      for feature_name in settings.input_features\n",
        "  }\n",
        "\n",
        "  history = model.fit(\n",
        "      x=features,\n",
        "      y=labels,\n",
        "      batch_size=settings.batch_size,\n",
        "      epochs=settings.number_epochs,\n",
        "  )\n",
        "\n",
        "  return Experiment(\n",
        "      name=experiment_name,\n",
        "      settings=settings,\n",
        "      model=model,\n",
        "      epochs=history.epoch,\n",
        "      metrics_history=pd.DataFrame(history.history),\n",
        "  )\n",
        "\n",
        "\n",
        "print('Defined the create_model and train_model functions.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq"
      },
      "source": [
        "## Define a plotting function\n",
        "\n",
        "The following [matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) function plots one or more curves, showing how various classification metrics change with each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF0BFRXTOeR3"
      },
      "outputs": [],
      "source": [
        "# @title Define the plotting function.\n",
        "def plot_experiment_metrics(experiment: Experiment, metrics: list[str]):\n",
        "  \"\"\"Plot a curve of one or more metrics for different epochs.\"\"\"\n",
        "  plt.figure(figsize=(12, 8))\n",
        "\n",
        "  for metric in metrics:\n",
        "    plt.plot(\n",
        "        experiment.epochs, experiment.metrics_history[metric], label=metric\n",
        "    )\n",
        "\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Metric value\")\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "print(\"Defined the plot_curve function.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-IXYVfvM4gD"
      },
      "source": [
        "## Invoke the creating, training, and plotting functions\n",
        "\n",
        "The following code specifies the hyperparameters, invokes the\n",
        "functions to create and train the model, then plots the results, including accuracy, precision, and recall.\n",
        "\n",
        "Classification threshold is set at .35. Try playing with the threshold, then the learning rate, to see what changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q82E6tS13O_2"
      },
      "outputs": [],
      "source": [
        "# Let's define our first experiment settings.\n",
        "settings = ExperimentSettings(\n",
        "    learning_rate=0.001,\n",
        "    number_epochs=60,\n",
        "    batch_size=100,\n",
        "    classification_threshold=0.35,\n",
        "    input_features=input_features,\n",
        ")\n",
        "\n",
        "metrics = [\n",
        "    keras.metrics.BinaryAccuracy(\n",
        "        name='accuracy', threshold=settings.classification_threshold\n",
        "    ),\n",
        "    keras.metrics.Precision(\n",
        "        name='precision', thresholds=settings.classification_threshold\n",
        "    ),\n",
        "    keras.metrics.Recall(\n",
        "        name='recall', thresholds=settings.classification_threshold\n",
        "    ),\n",
        "    keras.metrics.AUC(num_thresholds=100, name='auc'),\n",
        "]\n",
        "\n",
        "# Establish the model's topography.\n",
        "model = create_model(settings, metrics)\n",
        "\n",
        "# Train the model on the training set.\n",
        "experiment = train_model(\n",
        "    'baseline', model, train_features, train_labels, settings\n",
        ")\n",
        "\n",
        "# Plot metrics vs. epochs\n",
        "plot_experiment_metrics(experiment, ['accuracy', 'precision', 'recall'])\n",
        "plot_experiment_metrics(experiment, ['auc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfxkB-_vwUwq"
      },
      "source": [
        "AUC is calculated across all possible thresholds (in practice in the code above, 100 thresholds), while accuracy, precision, and recall are calculated for only the specified threshold. For this reason they are shown separately above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8y8vKBGsv0m"
      },
      "source": [
        "## Evaluate the model against the test set\n",
        "\n",
        "At the end of model training, you ended up with a certain accuracy against the *training set*. Invoke the following code cell to determine your model's accuracy against the *test set*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHh53BX44R94"
      },
      "outputs": [],
      "source": [
        "def evaluate_experiment(\n",
        "    experiment: Experiment, test_dataset: pd.DataFrame, test_labels: np.array\n",
        ") -> dict[str, float]:\n",
        "  features = {\n",
        "      feature_name: np.array(test_dataset[feature_name])\n",
        "      for feature_name in experiment.settings.input_features\n",
        "  }\n",
        "  return experiment.model.evaluate(\n",
        "      x=features,\n",
        "      y=test_labels,\n",
        "      batch_size=settings.batch_size,\n",
        "      verbose=0, # Hide progress bar\n",
        "      return_dict=True,\n",
        "  )\n",
        "\n",
        "\n",
        "def compare_train_test(experiment: Experiment, test_metrics: dict[str, float]):\n",
        "  print('Comparing metrics between train and test:')\n",
        "  for metric, test_value in test_metrics.items():\n",
        "    print('------')\n",
        "    print(f'Train {metric}: {experiment.get_final_metric_value(metric):.4f}')\n",
        "    print(f'Test {metric}:  {test_value:.4f}')\n",
        "\n",
        "\n",
        "# Evaluate test metrics\n",
        "test_metrics = evaluate_experiment(experiment, test_features, test_labels)\n",
        "compare_train_test(experiment, test_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku6nq9KDtL6u"
      },
      "source": [
        "It appears that the model, which achieved ~92% accuracy on the training data, still shows an accuracy of about 90% on the test data. Can we do better? Let's train a model using all seven available features and compare the AUC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72Mfkp3RxQii"
      },
      "outputs": [],
      "source": [
        "# Features used to train the model on.\n",
        "# Specify all features.\n",
        "all_input_features = [\n",
        "  'Eccentricity',\n",
        "  'Major_Axis_Length',\n",
        "  'Minor_Axis_Length',\n",
        "  ? Your code here\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xfdeJjoUNmTv"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "# Features used to train the model on.\n",
        "# Specify all features.\n",
        "all_input_features = [\n",
        "  'Eccentricity',\n",
        "  'Major_Axis_Length',\n",
        "  'Minor_Axis_Length',\n",
        "  'Area',\n",
        "  'Convex_Area',\n",
        "  'Perimeter',\n",
        "  'Extent',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hql2nxXqxuBg"
      },
      "source": [
        "## Train the full-featured model and calculate metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-85dcJ3ntocd"
      },
      "outputs": [],
      "source": [
        "settings_all_features = ExperimentSettings(\n",
        "    learning_rate=0.001,\n",
        "    number_epochs=60,\n",
        "    batch_size=100,\n",
        "    classification_threshold=0.5,\n",
        "    input_features=all_input_features,\n",
        ")\n",
        "\n",
        "# Modify the following definition of METRICS to generate\n",
        "# not only accuracy and precision, but also recall:\n",
        "metrics = [\n",
        "    keras.metrics.BinaryAccuracy(\n",
        "        name='accuracy',\n",
        "        threshold=settings_all_features.classification_threshold,\n",
        "    ),\n",
        "    keras.metrics.Precision(\n",
        "        name='precision',\n",
        "        thresholds=settings_all_features.classification_threshold,\n",
        "    ),\n",
        "    keras.metrics.Recall(\n",
        "        name='recall', thresholds=settings_all_features.classification_threshold\n",
        "    ),\n",
        "    keras.metrics.AUC(num_thresholds=100, name='auc'),\n",
        "]\n",
        "\n",
        "# Establish the model's topography.\n",
        "model_all_features = create_model(settings_all_features, metrics)\n",
        "\n",
        "# Train the model on the training set.\n",
        "experiment_all_features = train_model(\n",
        "    'all features',\n",
        "    model_all_features,\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    settings_all_features,\n",
        ")\n",
        "\n",
        "# Plot metrics vs. epochs\n",
        "plot_experiment_metrics(\n",
        "    experiment_all_features, ['accuracy', 'precision', 'recall']\n",
        ")\n",
        "plot_experiment_metrics(experiment_all_features, ['auc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5ndvrnjzXCo"
      },
      "source": [
        "## Evaluate full-featured model on test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BklcY6pyDrY"
      },
      "outputs": [],
      "source": [
        "test_metrics_all_features = evaluate_experiment(\n",
        "    experiment_all_features, test_features, test_labels\n",
        ")\n",
        "compare_train_test(experiment_all_features, test_metrics_all_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTr_boLBze2k"
      },
      "source": [
        "This second model has very similar train and test metrics, suggesting it overfit less to the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqgyfbXXawq4"
      },
      "source": [
        "# Comparing our two models\n",
        "\n",
        "With our simple experimentation framework, we can keep track of which experiments we ran, and what the results were. We can also define a helper function below which allows us to easily compare two or more models, both during training and when evaluated on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6Td7twEDa8t3"
      },
      "outputs": [],
      "source": [
        "#@title Define function to compare experiments\n",
        "\n",
        "def compare_experiment(experiments: list[Experiment],\n",
        "                       metrics_of_interest: list[str],\n",
        "                       test_dataset: pd.DataFrame,\n",
        "                       test_labels: np.array):\n",
        "  # Make sure that we have all the data we need.\n",
        "  for metric in metrics_of_interest:\n",
        "    for experiment in experiments:\n",
        "      if metric not in experiment.metrics_history:\n",
        "        raise ValueError(f'Metric {metric} not available for experiment {experiment.name}')\n",
        "\n",
        "  fig = plt.figure(figsize=(12, 12))\n",
        "  ax = fig.add_subplot(2, 1, 1)\n",
        "\n",
        "  colors = [f'C{i}' for i in range(len(experiments))]\n",
        "  markers = ['.', '*', 'd', 's', 'p', 'x']\n",
        "  marker_size = 10\n",
        "\n",
        "  ax.set_title('Train metrics')\n",
        "  for i, metric in enumerate(metrics_of_interest):\n",
        "    for j, experiment in enumerate(experiments):\n",
        "      plt.plot(experiment.epochs, experiment.metrics_history[metric], markevery=4,\n",
        "               marker=markers[i], markersize=marker_size, color=colors[j])\n",
        "\n",
        "  # Add custom legend to show what the colors and markers mean\n",
        "  legend_handles = []\n",
        "  for i, metric in enumerate(metrics_of_interest):\n",
        "    legend_handles.append(Line2D([0], [0], label=metric, marker=markers[i],\n",
        "                                 markersize=marker_size, c='k'))\n",
        "  for i, experiment in enumerate(experiments):\n",
        "    legend_handles.append(Line2D([0], [0], label=experiment.name, color=colors[i]))\n",
        "\n",
        "  ax.set_xlabel(\"Epoch\")\n",
        "  ax.set_ylabel(\"Metric value\")\n",
        "  ax.grid()\n",
        "  ax.legend(handles=legend_handles)\n",
        "\n",
        "  ax = fig.add_subplot(2, 1, 2)\n",
        "  spacing = 0.3\n",
        "  n_bars = len(experiments)\n",
        "  bar_width = (1 - spacing)/n_bars\n",
        "  for i, experiment in enumerate(experiments):\n",
        "    test_metrics = evaluate_experiment(experiment, test_dataset, test_labels)\n",
        "    x = np.arange(len(metrics_of_interest)) + bar_width * (i + 1/2 - n_bars/2)\n",
        "    ax.bar(x, [test_metrics[metric] for metric in metrics_of_interest], width=bar_width, label=experiment.name)\n",
        "  ax.set_xticks(np.arange(len(metrics_of_interest)), metrics_of_interest)\n",
        "\n",
        "  ax.set_title('Test metrics')\n",
        "  ax.set_ylabel('Metric value')\n",
        "  ax.set_axisbelow(True) # Put the grid behind the bars\n",
        "  ax.grid()\n",
        "  ax.legend()\n",
        "\n",
        "print('Defined function to compare experiments.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhbgA_FEayYU"
      },
      "outputs": [],
      "source": [
        "compare_experiment([experiment, experiment_all_features],\n",
        "                   ['accuracy', 'auc'],\n",
        "                   test_features, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKIuJGOTbNWz"
      },
      "source": [
        "Comparing the two models, both have AUC of ~.97-.98. There does not seem to be a large gain in model quality when adding the other four features, which makes sense, given that many of the features (area, perimeter, and convex area, for example) are interrelated."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}