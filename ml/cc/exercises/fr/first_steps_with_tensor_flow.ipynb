{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first_steps_with_tensor_flow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ajVM7rkoYXeL",
        "ci1ISxxrZ7v0",
        "copyright-notice"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "copyright-notice",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2017 Google LLC."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "id": "copyright-notice2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4f3CKqFUqL2-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " # Premiers pas avec TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "Bd2Zkk1LE2Zr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " **Objectifs d'apprentissage :**\n",
        "  * Concepts fondamentaux de TensorFlow\n",
        "  * Utiliser la classe `LinearRegressor` de TensorFlow pour prédire le prix médian des logements, au niveau des îlots urbains, sur la base d'une seule caractéristique d'entrée\n",
        "  * Évaluer la justesse des prédictions d'un modèle en utilisant la racine carrée de l'erreur quadratique moyenne (Root Mean Squared Error, RMSE)\n",
        "  * Améliorer la justesse d'un modèle en modifiant ses hyperparamètres"
      ]
    },
    {
      "metadata": {
        "id": "MxiIKhP4E2Zr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Les données sont basées sur le recensement de 1990 de l'État de Californie."
      ]
    },
    {
      "metadata": {
        "id": "6TjLjL9IU80G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ## Configuration\n",
        "Dans cette première cellule, vous allez charger les bibliothèques nécessaires."
      ]
    },
    {
      "metadata": {
        "id": "rVFf5asKE2Zt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division, print_function\n",
        "from future.utils import iteritems\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ipRyUHjhU80Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Vous allez ensuite charger votre ensemble de données."
      ]
    },
    {
      "metadata": {
        "id": "9ivCDWnwE2Zx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "california_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vVk_qlG6U80j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Vous allez mélanger les données de manière aléatoire, pour être sûr d'éviter les effets de classement pathologique pouvant nuire aux performances de la descente de gradient stochastique. Vous allez, en outre, mettre à l'échelle la valeur `median_house_value` pour qu'elle soit exprimée en milliers, de sorte que son apprentissage soit un peu plus facile avec des taux dans l'intervalle utilisé généralement."
      ]
    },
    {
      "metadata": {
        "id": "r0eVyguIU80m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "california_housing_dataframe = california_housing_dataframe.reindex(\n",
        "    np.random.permutation(california_housing_dataframe.index))\n",
        "california_housing_dataframe[\"median_house_value\"] /= 1000.0\n",
        "california_housing_dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HzzlSs3PtTmt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ## Analyse des données\n",
        "\n",
        "Il est conseillé de se familiariser avec les données avant de les exploiter.\n",
        "\n",
        "Nous allons imprimer un bref résumé de quelques statistiques utiles sur chaque colonne : nombre d'exemples, moyenne, écart type, maximum, minimum et divers quantiles."
      ]
    },
    {
      "metadata": {
        "id": "gzb10yoVrydW",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "california_housing_dataframe.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lr6wYl2bt2Ep",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ## Construction du premier modèle\n",
        "\n",
        "Dans cet exercice, vous allez essayer de prédire la valeur médiane d'un logement (`median_house_value`) qui deviendra l'étiquette (que l'on désigne parfois également sous le nom de cible). Vous utiliserez le nombre de pièces (`total_rooms`) comme caractéristique d'entrée.\n",
        "\n",
        "**REMARQUE :** Les données se situent au niveau de l'îlot urbain. Cette caractéristique représente donc le nombre total de pièces dans cet îlot.\n",
        "\n",
        "Pour entraîner le modèle, vous allez utiliser l'interface [LinearRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor) fournie par l'API [Estimator](https://www.tensorflow.org/get_started/estimator) de TensorFlow. Cette API s'occupe d'une bonne partie des mécanismes sous-jacents du modèle de bas niveau, et propose des méthodes pratiques pour effectuer les tâches d'apprentissage du modèle, d'évaluation et d'inférence."
      ]
    },
    {
      "metadata": {
        "id": "0cpcsieFhsNI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Étape 1 : Définir les caractéristiques et configurer les colonnes de caractéristiques"
      ]
    },
    {
      "metadata": {
        "id": "EL8-9d4ZJNR7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Pour qu'il soit possible d'importer les données d'apprentissage dans TensorFlow, vous devez spécifier le type de données qui se trouve dans chaque caractéristique. Vous utiliserez principalement deux types de données pour cet exercice et les suivants :\n",
        "\n",
        "* **Données catégorielles** : il s'agit de données textuelles. L'ensemble de données immobilières utilisé dans cet exercice ne contient aucune caractéristique catégorielle. Ce type de données pourrait être le style de logement, le contenu d'une annonce immobilière…\n",
        "\n",
        "* **Données numériques** : données représentant un nombre (entier ou à virgule flottante) et que vous souhaitez traiter comme tel. Comme nous le verrons par la suite, vous pouvez, dans certains cas, traiter des données numériques (un code postal, par exemple), comme si elles étaient de type catégoriel.\n",
        "\n",
        "Dans TensorFlow, le type de données d'une caractéristique est indiqué à l'aide d'une construction appelée **colonne de caractéristiques**. Les colonnes de ce type ne stockent qu'une description des données de la caractéristique ; elles ne contiennent pas les données proprement dites.\n",
        "\n",
        "Dans un premier temps, vous allez simplement utiliser une caractéristique d'entrée numérique, `total_rooms`. Le code suivant extrait les données `total_rooms` de l'ensemble `california_housing_dataframe` et définit la colonne de caractéristiques à l'aide de `numeric_column`, qui précise que ses données sont numériques :"
      ]
    },
    {
      "metadata": {
        "id": "rhEbFCZ86cDZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the input feature: total_rooms.\n",
        "my_feature = california_housing_dataframe[[\"total_rooms\"]]\n",
        "\n",
        "# Configure a numeric feature column for total_rooms.\n",
        "feature_columns = [tf.feature_column.numeric_column(\"total_rooms\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K_3S8teX7Rd2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " **REMARQUE :** Les données `total_rooms` se présentent sous la forme d'un tableau à une seule dimension (une liste du nombre total de pièces pour chaque îlot urbain). Il s'agit de la forme par défaut pour `numeric_column`. Par conséquent, elle ne doit pas être transmise en tant qu'argument."
      ]
    },
    {
      "metadata": {
        "id": "UMl3qrU5MGV6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Étape 2 : Définir la cible"
      ]
    },
    {
      "metadata": {
        "id": "cw4nrfcB7kyk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Vous allez ensuite définir la cible, à savoir la valeur médiane d'un logement (`median_house_value`). Ici encore, vous pouvez extraire cette donnée de l'ensemble `california_housing_dataframe` :"
      ]
    },
    {
      "metadata": {
        "id": "l1NvvNkH8Kbt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the label.\n",
        "targets = california_housing_dataframe[\"median_house_value\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4M-rTFHL2UkA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Étape 3 : Configurer la classe LinearRegressor"
      ]
    },
    {
      "metadata": {
        "id": "fUfGQUNp7jdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Vous allez ensuite configurer un modèle de régression linéaire à l'aide de LinearRegressor. Vous allez entraîner ce modèle à l'aide de `GradientDescentOptimizer`, qui met en œuvre une descente de gradient stochastique par mini-lots. L'argument `learning_rate` détermine la taille du pas de gradient.\n",
        "\n",
        "**REMARQUE :** Par mesure de précaution, un [bornement de la norme du gradient] (https://developers.google.com/machine-learning/glossary/#gradient_clipping) est également appliqué à l'optimiseur via `clip_gradients_by_norm`. Le bornement de la norme du gradient permet de s'assurer que la magnitude des gradients reste dans des limites acceptables au cours de l'apprentissage, sans quoi la descente de gradient risque d'échouer. "
      ]
    },
    {
      "metadata": {
        "id": "ubhtW-NGU802",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use gradient descent as the optimizer for training the model.\n",
        "my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\n",
        "my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "\n",
        "# Configure the linear regression model with our feature columns and optimizer.\n",
        "# Set a learning rate of 0.0000001 for Gradient Descent.\n",
        "linear_regressor = tf.estimator.LinearRegressor(\n",
        "    feature_columns=feature_columns,\n",
        "    optimizer=my_optimizer\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-0IztwdK2f3F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Étape 4 : Définir la fonction d'entrée"
      ]
    },
    {
      "metadata": {
        "id": "S5M5j6xSCHxx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Pour importer les données immobilières de l'État de Californie dans `LinearRegressor`, une fonction d'entrée doit être définie. Cette fonction indique non seulement à TensorFlow comment effectuer le prétraitement des données, mais aussi comment les mettre en lots, les lire en mode aléatoire et les répéter pendant l'entraînement du modèle.\n",
        "\n",
        "Vous allez, tout d'abord, convertir les données de la caractéristique *pandas* en un dictionnaire de tableaux NumPy. Vous pourrez ensuite utiliser l'[API Dataset] (https://www.tensorflow.org/programmers_guide/datasets) de TensorFlow pour construire un objet d'ensemble de données à partir de ces données, puis scinder ces dernières dans des lots de `batch_size`, de sorte qu'elles soient répétées pour le nombre indiqué d'itérations (num_epochs). \n",
        "\n",
        "**REMARQUE :** Lorsque la valeur par défaut de `num_epochs=None` est transmise à `repeat()`, les données d'entrée sont répétées indéfiniment.\n",
        "\n",
        "Ensuite, si `shuffle` est défini sur `True`, les données seront lues de manière aléatoire, de façon à être transmises aléatoirement au modèle au cours de l'apprentissage. L'argument `buffer_size` indique la taille de l'ensemble de données à partir duquel `shuffle` sera échantillonné de manière aléatoire.\n",
        "\n",
        "Pour terminer, la fonction d'entrée construit un itérateur pour l'ensemble de données et renvoie le lot de données suivant à LinearRegressor."
      ]
    },
    {
      "metadata": {
        "id": "RKZ9zNcHJtwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
        "    \"\"\"Trains a linear regression model of one feature.\n",
        "  \n",
        "    Args:\n",
        "      features: pandas DataFrame of features\n",
        "      targets: pandas DataFrame of targets\n",
        "      batch_size: Size of batches to be passed to the model\n",
        "      shuffle: True or False. Whether to shuffle the data.\n",
        "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
        "    Returns:\n",
        "      Tuple of (features, labels) for next data batch\n",
        "    \"\"\"\n",
        "  \n",
        "    # Convert pandas data into a dict of np arrays.\n",
        "    features = {key: np.array(value) for key, value in iteritems(features)}                                           \n",
        " \n",
        "    # Construct a dataset, and configure batching/repeating.\n",
        "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    # Shuffle the data, if specified.\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=10000)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    features, labels = ds.make_one_shot_iterator().get_next()\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wwa6UeA1V5F_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " **REMARQUE :** Vous continuerez à utiliser cette fonction d'entrée dans les prochains exercices. Pour en savoir plus sur les fonctions d'entrée et l'API `Dataset`, consultez le [Guide du programmeur de TensorFlow] (en anglais) (https://www.tensorflow.org/programmers_guide/datasets)."
      ]
    },
    {
      "metadata": {
        "id": "4YS50CQb2ooO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Étape 5 : Entraîner le modèle"
      ]
    },
    {
      "metadata": {
        "id": "yP92XkzhU803",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Vous pouvez maintenant appeler `train()` sur `linear_regressor` pour entraîner le modèle. Vous allez encapsuler `my_input_fn` dans un `lambda`\n",
        "afin de pouvoir transmettre `my_feature` et `target` comme arguments (pour en savoir plus, reportez-vous à ce [didacticiel sur la fonction d'entrée TensorFlow](https://www.tensorflow.org/get_started/input_fn#passing_input_fn_data_to_your_model)). Pour commencer, vous allez effectuer\n",
        "l'apprentissage pour 100 pas."
      ]
    },
    {
      "metadata": {
        "id": "5M-Kt6w8U803",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "linear_regressor.train(\n",
        "    lambda: my_input_fn(my_feature, targets),\n",
        "    steps=100\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Nwxqxlx2sOv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Étape 6 : Évaluer le modèle"
      ]
    },
    {
      "metadata": {
        "id": "KoDaF2dlJQG5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Vous allez faire des prédictions sur les données d'apprentissage afin de déterminer dans quelle mesure elles sont adaptées à votre modèle au cours de l'apprentissage.\n",
        "\n",
        "**REMARQUE :** L'erreur d'apprentissage mesure à quel point votre modèle est adapté aux données d'apprentissage. En revanche, elle ne mesure **_pas_** la qualité de **_généralisation du modèle aux nouvelles données_**. Au cours des prochains exercices, vous examinerez comment scinder les données afin d'évaluer la capacité de généralisation de votre modèle.\n"
      ]
    },
    {
      "metadata": {
        "id": "pDIxp6vcU809",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create an input function for predictions.\n",
        "# Note: Since we're making just one prediction for each example, we don't \n",
        "# need to repeat or shuffle the data here.\n",
        "prediction_input_fn =lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)\n",
        "\n",
        "# Create a numpy array of predicted values from the prediction items generated by linear_regressor `predict` method\n",
        "predictions = np.array(\n",
        "    [item['predictions'][0] for item in linear_regressor.predict(input_fn=prediction_input_fn)]\n",
        ")\n",
        "\n",
        "# Print Mean Squared Error and Root Mean Squared Error.\n",
        "mean_squared_error = metrics.mean_squared_error(predictions, targets)\n",
        "root_mean_squared_error = math.sqrt(mean_squared_error)\n",
        "print(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\n",
        "print(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AKWstXXPzOVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " S'agit-il d'un bon modèle ? Comment évaluer l'importance de l'erreur ?\n",
        "\n",
        "Une erreur quadratique moyenne (MSE) pouvant être difficile à interpréter, on tient plutôt compte de la racine carrée de l'erreur quadratique\n",
        "moyenne (RMSE). Une propriété intéressante de RMSE est la possibilité de l'interpréter sur la même échelle que les cibles d'origine.\n",
        "\n",
        "Comparons la RMSE à la différence des valeurs minimale et maximale de nos cibles :"
      ]
    },
    {
      "metadata": {
        "id": "7UwqGbbxP53O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "min_house_value = california_housing_dataframe[\"median_house_value\"].min()\n",
        "max_house_value = california_housing_dataframe[\"median_house_value\"].max()\n",
        "min_max_difference = max_house_value - min_house_value\n",
        "\n",
        "print(\"Min. Median House Value: %0.3f\" % min_house_value)\n",
        "print(\"Max. Median House Value: %0.3f\" % max_house_value)\n",
        "print(\"Difference between Min. and Max.: %0.3f\" % min_max_difference)\n",
        "print(\"Root Mean Squared Error: %0.3f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JigJr0C7Pzit",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " L'erreur couvre pratiquement la moitié de l'intervalle des valeurs cibles. Peut-on faire mieux ?\n",
        "\n",
        "C'est la question qui agace tous les développeurs de modèle. Élaborons quelques stratégies de base pour réduire l'erreur de modèle.\n",
        "\n",
        "Pour commencer, vous pouvez examiner dans quelle mesure les prédictions répondent aux cibles qui ont été définies, en termes de statistiques récapitulatives globales."
      ]
    },
    {
      "metadata": {
        "id": "941nclxbzqGH",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "calibration_data = pd.DataFrame()\n",
        "calibration_data[\"predictions\"] = pd.Series(predictions)\n",
        "calibration_data[\"targets\"] = pd.Series(targets)\n",
        "calibration_data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2-bf8Hq36y8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Ces informations peuvent s'avérer bien utiles. Comment peut-on comparer la valeur moyenne à la valeur RMSE du modèle ? Qu'en est-il des divers quantiles ?\n",
        "\n",
        "Vous pouvez également visualiser les données et la ligne qui a été apprise. Pour rappel, la régression linéaire sur une seule caractéristique peut être représentée par une droite transformant l'entrée *x* en la sortie *y*.\n",
        "\n",
        "Vous allez d'abord obtenir un échantillon aléatoire uniforme des données, de manière à créer un diagramme de dispersion lisible."
      ]
    },
    {
      "metadata": {
        "id": "SGRIi3mAU81H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample = california_housing_dataframe.sample(n=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-JwuJBKU81J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Vous allez ensuite représenter, sous forme graphique, la ligne qui a été apprise, en partant de la pondération de caractéristique et du biais du modèle, superposée au diagramme de dispersion. La ligne sera affichée en rouge."
      ]
    },
    {
      "metadata": {
        "id": "7G12E76-339G",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the min and max total_rooms values.\n",
        "x_0 = sample[\"total_rooms\"].min()\n",
        "x_1 = sample[\"total_rooms\"].max()\n",
        "\n",
        "# Retrieve the final weight and bias generated during training.\n",
        "weight = linear_regressor.get_variable_value('linear/linear_model/total_rooms/weights')[0]\n",
        "bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
        "\n",
        "# Get the predicted median_house_values for the min and max total_rooms values.\n",
        "y_0 = weight * x_0 + bias \n",
        "y_1 = weight * x_1 + bias\n",
        "\n",
        "# Plot our regression line from (x_0, y_0) to (x_1, y_1).\n",
        "plt.plot([x_0, x_1], [y_0, y_1], c='r')\n",
        "\n",
        "# Label the graph axes.\n",
        "plt.ylabel(\"median_house_value\")\n",
        "plt.xlabel(\"total_rooms\")\n",
        "\n",
        "# Plot a scatter plot from our data sample.\n",
        "plt.scatter(sample[\"total_rooms\"], sample[\"median_house_value\"])\n",
        "\n",
        "# Display graph.\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t0lRt4USU81L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Cette ligne initiale semble très éloignée. Voyez s'il est possible de revenir aux statistiques récapitulatives et d'examiner les mêmes informations qui y sont encodées.\n",
        "\n",
        "Ces évaluations d'intégrité laissent supposer qu'il doit être possible de trouver une bien meilleure ligne"
      ]
    },
    {
      "metadata": {
        "id": "AZWF67uv0HTG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ## Modifier les hyperparamètres du modèle\n",
        "Dans le cadre de cet exercice, tout le code ci-dessus a été placé dans une seule fonction pour plus de facilité. Vous pouvez appeler cette fonction avec différents paramètres pour visualiser les effets.\n",
        "\n",
        "Dans cette fonction, vous travaillerez dans 10 périodes réparties uniformément, afin de pouvoir observer l'amélioration du modèle à chaque période.\n",
        "\n",
        "Pour chaque période, vous allez calculer et représenter graphiquement la perte d'apprentissage. Cela peut vous aider à déterminer si un modèle a convergé ou si des itérations supplémentaires sont nécessaires.\n",
        "\n",
        "Vous allez également représenter graphiquement les valeurs de biais et de pondération de caractéristique apprises par le modèle au fil du temps. Il s'agit d'une autre méthode pour visualiser la convergence des éléments."
      ]
    },
    {
      "metadata": {
        "id": "wgSMeD5UU81N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(learning_rate, steps, batch_size, input_feature=\"total_rooms\"):\n",
        "  \"\"\"Trains a linear regression model of one feature.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    input_feature: A `string` specifying a column from `california_housing_dataframe`\n",
        "      to use as input feature.\n",
        "  \"\"\"\n",
        "  \n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "\n",
        "  my_feature = input_feature\n",
        "  my_feature_data = california_housing_dataframe[[my_feature]]\n",
        "  my_label = \"median_house_value\"\n",
        "  targets = california_housing_dataframe[my_label]\n",
        "\n",
        "  # Create feature columns.\n",
        "  feature_columns = [tf.feature_column.numeric_column(my_feature)]\n",
        "  \n",
        "  # Create input functions.\n",
        "  training_input_fn = lambda:my_input_fn(my_feature_data, targets, batch_size=batch_size)\n",
        "  prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)\n",
        "  \n",
        "  # Create a linear regressor object.\n",
        "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  linear_regressor = tf.estimator.LinearRegressor(\n",
        "      feature_columns=feature_columns,\n",
        "      optimizer=my_optimizer\n",
        "  )\n",
        "\n",
        "  # Set up to plot the state of our model's line each period.\n",
        "  plt.figure(figsize=(15, 6))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title(\"Learned Line by Period\")\n",
        "  plt.ylabel(my_label)\n",
        "  plt.xlabel(my_feature)\n",
        "  sample = california_housing_dataframe.sample(n=300)\n",
        "  plt.scatter(sample[my_feature], sample[my_label])\n",
        "  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"RMSE (on training data):\")\n",
        "  root_mean_squared_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    linear_regressor.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "    # Take a break and compute predictions.\n",
        "    predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n",
        "    predictions = np.array([item['predictions'][0] for item in predictions])\n",
        "    \n",
        "    # Compute loss.\n",
        "    root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(predictions, targets))\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, root_mean_squared_error))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    root_mean_squared_errors.append(root_mean_squared_error)\n",
        "    # Finally, track the weights and biases over time.\n",
        "    # Apply some math to ensure that the data and line are plotted neatly.\n",
        "    y_extents = np.array([0, sample[my_label].max()])\n",
        "    \n",
        "    weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]\n",
        "    bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
        "\n",
        "    x_extents = (y_extents - bias) / weight\n",
        "    x_extents = np.maximum(np.minimum(x_extents,\n",
        "                                      sample[my_feature].max()),\n",
        "                           sample[my_feature].min())\n",
        "    y_extents = weight * x_extents + bias\n",
        "    plt.plot(x_extents, y_extents, color=colors[period]) \n",
        "  print(\"Model training finished.\")\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.ylabel('RMSE')\n",
        "  plt.xlabel('Periods')\n",
        "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot(root_mean_squared_errors)\n",
        "\n",
        "  # Output a table with calibration data.\n",
        "  calibration_data = pd.DataFrame()\n",
        "  calibration_data[\"predictions\"] = pd.Series(predictions)\n",
        "  calibration_data[\"targets\"] = pd.Series(targets)\n",
        "  display.display(calibration_data.describe())\n",
        "\n",
        "  print(\"Final RMSE (on training data): %0.2f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kg8A4ArBU81Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ## Tâche 1 : Obtenir une valeur RMSE inférieure ou égale à 180\n",
        "\n",
        "Modifiez les hyperparamètres du modèle pour améliorer le coût et obtenir une meilleure correspondance avec la distribution cible.\n",
        "Si, après environ cinq minutes, vous ne parvenez toujours pas à obtenir une valeur RMSE de 180, vérifiez la solution pour afficher une combinaison applicable."
      ]
    },
    {
      "metadata": {
        "id": "UzoZUSdLIolF",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    learning_rate=0.00001,\n",
        "    steps=100,\n",
        "    batch_size=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ajVM7rkoYXeL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Solution\n",
        "\n",
        "Cliquez ci-dessous pour afficher une solution."
      ]
    },
    {
      "metadata": {
        "id": "T3zmldDwYy5c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    learning_rate=0.00002,\n",
        "    steps=500,\n",
        "    batch_size=5\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8H0_D4vYa49",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Il ne s'agit là que d'une configuration parmi d'autres ; d'autres combinaisons de paramètres peuvent également donner de bons résultats. Notez que le but de cet exercice n'est pas de trouver le paramètre *optimal*, mais bien de vous aider à percevoir en quoi le réglage de la configuration du modèle affecte la qualité de prédiction."
      ]
    },
    {
      "metadata": {
        "id": "QU5sLyYTqzqL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Existe-t-il une méthode heuristique standard en matière de réglage de modèle ?\n",
        "\n",
        "Il s'agit là d'une question courante. En bref, on peut dire que les effets des différents hyperparamètres dépendent des données. Il n'existe donc pas de règles absolues. Vous devez effectuer des tests sur les données !\n",
        "\n",
        "Cela étant dit, voici quelques règles empiriques qui peuvent s'avérer utiles :\n",
        "\n",
        " * L'erreur d'apprentissage doit diminuer régulièrement, selon une pente abrupte dans un premier temps, pour finalement se stabiliser à mesure que l'apprentissage converge.\n",
        " * Si l'apprentissage n'a pas convergé, essayez de l'exécuter plus longtemps.\n",
        " * Si l'erreur d'apprentissage diminue trop lentement, augmenter le taux d'apprentissage permettra peut-être d'accélérer la diminution.\n",
        "   * Cependant, il arrive parfois que l'inverse se produise si le taux d'apprentissage est trop élevé.\n",
        " * Si l'erreur d'apprentissage varie sensiblement, essayez de diminuer le taux d'apprentissage.\n",
        "   * Une bonne méthode consiste généralement à diminuer le taux d'apprentissage tout en augmentant le nombre de pas ou la taille du lot.\n",
        " * Des lots de très petite taille peuvent également entraîner une instabilité. Commencez par des valeurs telles que 100 ou 1 000, et continuez à réduire la taille jusqu'à ce que vous constatiez une dégradation.\n",
        "\n",
        "Pour rappel, vous ne devez pas suivre à la lettre ces règles empiriques, car les effets dépendent des données. Vous devez toujours essayer une méthode et vérifier ensuite le résultat."
      ]
    },
    {
      "metadata": {
        "id": "GpV-uF_cBCBU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ## Tâche 2 : Essayer une autre caractéristique\n",
        "\n",
        "Voyons s'il est possible d'obtenir un meilleur résultat en remplaçant la caractéristique `total_rooms` par `population`.\n",
        "\n",
        "Ne consacrez pas plus de cinq minutes à cette tâche."
      ]
    },
    {
      "metadata": {
        "id": "YMyOxzb0ZlAH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ci1ISxxrZ7v0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Solution\n",
        "\n",
        "Cliquez ci-dessous pour afficher une solution."
      ]
    },
    {
      "metadata": {
        "id": "SjdQQCduZ7BV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    learning_rate=0.00002,\n",
        "    steps=1000,\n",
        "    batch_size=5,\n",
        "    input_feature=\"population\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}