{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rl-deep-q-networks.ipynb","version":"0.3.2","provenance":[{"file_id":"/piper/depot/google3/engedu/ml/self_study/rl/rl-deep-q-learning.ipynb?workspaceId=karangill:mlcc::citc","timestamp":1552578826835}],"collapsed_sections":["9EjQt_o9Xf_L","3_HCbNWf3RPP"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9EjQt_o9Xf_L"},"source":["## Copyright 2019 Google LLC."]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"oXzTW-CnXf_Q","colab":{}},"source":["#@title\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ONq0GBqct2J-"},"source":["# 4. Deep Q-Learning"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"E_lZYMFUuHJT"},"source":["In this Colab, you will combine Q-learning with neural networks to create a powerful technique, called Deep Q-Learning (DQN)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"W_Y-7HdxuFTV"},"source":["## Motivation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xwbwfeFot15w"},"source":["In the last Colab, you learned tabular Q-learning. Your Q-table required an entry for every combination of state and action. However, for complex environments that have many states and actions, the Q table's size becomes massive. Instead of a Q-table, you can predict Q-values using a neural network. This application of deep learning to Q-learning is called DQN."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pqf2mYg2kM3j"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"urxZETIjO0c4","colab_type":"text"},"source":["Run the following cell to set up Google Analytics for the Colab. Data from  Google Analytics helps improve the Colab."]},{"cell_type":"code","metadata":{"id":"ngfeEbGgO3rN","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Set up Google Analytics for Colab\n","%reset -f\n","import uuid\n","client_id = uuid.uuid4()\n","\n","import requests\n","\n","# Bundle up reporting into a function.\n","def report_execution():\n","  requests.post('https://www.google-analytics.com/collect', \n","                data=('v=1'\n","                      '&tid=UA-48865479-3'\n","                      '&cid={}'\n","                      '&t=event'\n","                      '&ec=cell'            # <-- event type\n","                      '&ea=execute'         # <-- event action\n","                      '&el=rl-deep-q-learning'   # <-- event label\n","                      '&ev=1'               # <-- event value\n","                      '&an=bundled'.format(client_id)))\n","\n","from IPython import get_ipython\n","get_ipython().events.register('post_execute', report_execution)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"67cvduNXkOvl"},"source":["Run the following cell to import libraries and set up the environment:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nksx4wVs0Dro","colab":{}},"source":["import gym\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","\n","EPSILON_MIN = 0.01\n","CHECK_SUCCESS_INTERVAL = 100\n","\n","env = gym.make('FrozenLake-v0')\n","\n","num_states = env.observation_space.n\n","num_actions = env.action_space.n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TC93YAHtMUTp"},"source":["## Define Neural Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SrkGBnRVMUuK"},"source":["You will implement the same epsilon-greedy policy environment. However, instead of storing Q-values in a table, you will use a neural network to generate the Q-values."]},{"cell_type":"markdown","metadata":{"id":"Ig3VKjezKyD2","colab_type":"text"},"source":["The input to the neural net is the state. In FrozenLake, represent the state using [one-hot encoding](https://developers.google.com/machine-learning/crash-course/representation/feature-engineering). For example, encode the state `10` by running the following code:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QKw7aGg6N7ve","colab":{}},"source":["np.identity(num_states)[10:10+1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmWVcBGh78Fn","colab_type":"text"},"source":["Define a function to create encode states as one-hot vectors:"]},{"cell_type":"code","metadata":{"id":"1quifpKJ8Bk8","colab_type":"code","colab":{}},"source":["def one_hot_encode_state(state):\n","  \"\"\"Args:\n","       state: An integer representing the agent's state.\n","     Returns:\n","       A one-hot encoded vector of the input `state`.\n","  \"\"\"\n","  return(np.identity(num_states)[state:state+1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"28yX_0-LN2tk"},"source":["The input to the neural net is a vector of length 16. The output is a vector of Q-values for each action. Since there are 4 actions, the output is a vector of length 4.\n","\n","Define a nonlinear neural net with 16 inputs and 4 outputs by using the TF Keras API. The neural net has these characteristics:\n","\n","* Uses `relu` activation function.\n","* Is initialized with small positive weights. Ideally, you should use known good initial values for the weights. The initialization with positive values is a workaround.\n","* Does not use biases. To understand why, suppose you used biases. Now, for an input $s_1$, the neural network predicts $Q(s_1,a_1)$ by transforming $s_1$ to $f(s_1)$. Then the output neuron for $a_1$ adds a bias, $b_{a_{1}}$,  as follows:\n","$$Q(s_1,a_1) = f(s_1) + b_{a_{1}}$$\n","Similarly, for state $s_2$, the prediction $Q(s_2,a_1)$ adds the same bias $b_{a_{1}}$ because the action (and thus the output neuron) remains the same:\n","$$Q(s_2,a_1) = f(s_2) + b_{a_{1}}$$\n","Therefore, for the same action, Q-value predictions depend on the same bias, even if the input state varies. Training to predict the Q-value for $(s_1,a_1)$ will change $b_{a_1}$. However, changing $b_{a_1}$ will change the predicted Q-values for $(s_2,a_1)$, resulting in wrong Q-values. Therefore, do not use biases.\n","\n","Complete the neural net definition in the following cell as described. Then run the cell. For the solution, view the next cell."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CRmwy-7wOgo9","colab":{}},"source":["def define_model(learning_rate):\n","  '''Returns a shallow neural net defined using tf.keras.\n","  Args:\n","    learning_rate: optimizer learning rate\n","  Returns:\n","    model: A shallow neural net defined using tf.keras input dimension equal to\n","    num_states and output dimension equal to num_actions.\n","  '''\n","  model = keras.Sequential()\n","  # === Complete this section by replacing the \"...\" with appropriate values ===\n","  # model.add(keras.layers.Dense(units = ...,\n","  #                              input_dim = ...,\n","  #                              activation = ...,\n","  #                              use_bias = False,\n","  #                              # next line initializes weights with small positive values \n","  #                              kernel_initializer = keras.initializers.RandomUniform(minval=1e-5, maxval=0.05)\n","  #                             ))\n","  # ============================================================================\n","  model.compile(optimizer = keras.optimizers.SGD(lr = learning_rate),\n","                loss = 'mse')\n","  return(model)\n","\n","learning_rate = 0.1\n","model = define_model(learning_rate)\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aI4jUM14oCTN","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Solution (double-click to view code)\n","def define_model(learning_rate):\n","  '''Returns a shallow neural net defined using tf.keras.\n","  Args:\n","    learning_rate: optimizer learning rate\n","  Returns:\n","    model: A shallow neural net defined using tf.keras input dimension equal to\n","    num_states and output dimension equal to num_actions.\n","  '''\n","  model = []\n","  model = keras.Sequential()\n","  model.add(keras.layers.Dense(units = num_actions,\n","                               input_dim = num_states,\n","                               activation = 'relu',\n","                               use_bias = False,\n","                               kernel_initializer = keras.initializers.RandomUniform(minval=1e-5, maxval=0.05)\n","                              ))\n","  model.compile(optimizer = keras.optimizers.SGD(lr = learning_rate),\n","                loss = 'mse')\n","  return(model)\n","\n","learning_rate = 0.1\n","model = define_model(learning_rate)\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9uKEl0omCt_x","colab_type":"text"},"source":["## Calculate Q-Values from Neural Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hFezuuVFQ9MZ"},"source":["You can use your neural network to predict Q-values for any state. For example, predict Q-values for state 5 by running the following cell. Since your neural network has not been trained, these predicted Q-values are inaccurate."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SMQLS88-Q8Sk","colab":{}},"source":["model.predict(one_hot_encode_state(5))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8ujOc1lkkgy","colab_type":"text"},"source":["Complete the following cell to implement a function (identical to the previous Colab) that returns an action using an epsilon greedy policy. Then run the cell. For the solution, view the next cell."]},{"cell_type":"code","metadata":{"id":"7Kk7ZOmwKzIf","colab_type":"code","colab":{}},"source":["def policy_eps_greedy(env, q_values, epsilon):\n","  \"\"\"Select action given Q-values using epsilon-greedy algorithm.\n","  Args:\n","    q_values: q_values for all possible actions from a state.\n","    epsilon: Current value of epsilon used to select action using epsilon-greedy\n","             algorithm.\n","  Returns:\n","    action: action to take from the state.\n","  \"\"\"\n","  # === Complete this section by replacing the \"...\" with appropriate values ===\n","  # if(np.random.rand() < ...):\n","  #   action = ...\n","  # else:\n","  #   action = ...\n","  # ============================================================================\n","  return action"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMoJimVLpSlQ","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n","def policy_eps_greedy(env, q_values, epsilon):\n","  \"\"\"Select action given Q-values using epsilon-greedy algorithm.\n","  Args:\n","    q_values: q_values for all possible actions from a state.\n","    epsilon: Current value of epsilon used to select action using epsilon-greedy\n","             algorithm.\n","  Returns:\n","    action: action to take from the state.\n","  \"\"\"\n","  if(np.random.rand() < epsilon):\n","    action = env.action_space.sample()\n","  else:\n","    action = np.argmax(q_values)\n","  return action"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XdYfD-PSRsWB"},"source":["In deep Q-learning, the neural network replaces the Q-table. To demonstrate how, run a full training step using the neural network.\n","\n","First, reset the environment and calculate Q-values for the starting state:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Yv3eTPKESFDg","colab":{}},"source":["state = env.reset()\n","q_values = model.predict(one_hot_encode_state(state))\n","print(\"Q-values for state \" + str(state) + \" :\\n\" + str(q_values))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"euFA4Rs0DU6i","colab_type":"text"},"source":["Each Q-value represents the approximated return from taking the corresponding action and then following a greedy policy. Therefore, when Q-values are accurate, choosing the action with the highest Q-value will maximize return.\n","\n","Using the Q-values, select an action using an epsilon-greedy policy. Take the action and record the next state and reward."]},{"cell_type":"code","metadata":{"id":"9WpRbnLzDQdL","colab_type":"code","colab":{}},"source":["epsilon = 0.5 # assume some value of epsilon\n","\n","action = policy_eps_greedy(env, q_values, epsilon)\n","state_new, reward, _, _ = env.step(action)\n","\n","print(\"action:\", action, \", next state:\", state_new, \", reward:\", reward)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_carvIRkXDpB","colab_type":"text"},"source":["Calculate the target Q-value by completing the following cell to define a function. The formula for the returned Q-value is:\n","\n","$$\n","  r(s,a)\n","      + \\gamma \\displaystyle \\max_{\\substack{a_1}} Q(s_1,a_1)\n","$$\n","\n","This function is similar to the Bellman update in the previous Colab, except for the use of a neural network."]},{"cell_type":"code","metadata":{"id":"0o7bIJuHXH8P","colab_type":"code","colab":{}},"source":["def bellman_update(reward, discount_factor, model, state_new):\n","  # =========== Complete this section by replacing the \"...\" ===================\n","  # return ...\n","  # ============================================================================"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vc4WCwaLpyNI","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n","def bellman_update(reward, discount_factor, model, state_new):\n","  return reward + discount_factor * \\\n","                  np.max(model.predict(one_hot_encode_state(state_new)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sEAOTJ5TEURA","colab_type":"text"},"source":["Calculate target Q-values by calling the function `bellman_update`:"]},{"cell_type":"code","metadata":{"id":"-dqiXnSDEUhH","colab_type":"code","colab":{}},"source":["discount_factor = 0.99\n","\n","print(\"Q-values before update for state \" + str(state) + \" :\\n\" + str(q_values))\n","target_q_values = q_values\n","target_q_values[0, action] = bellman_update(reward, discount_factor, model,\n","                                           state_new)\n","\n","print(\"Q-values after update for state \" + str(state) + \" :\\n\" + str(target_q_values))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jxck8BVVFFIo","colab_type":"text"},"source":["Notice that only the Q-value corresponding to the action taken changes after the update. The updated Q-values become the \"target\" label that the neural network uses to train.\n","\n","Train the neural network to predict the target Q-values:"]},{"cell_type":"code","metadata":{"id":"KRBdtUtWFFS7","colab_type":"code","colab":{}},"source":["model.fit(one_hot_encode_state(state), target_q_values, verbose = True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CgNxvZRG1GQy"},"source":["To summarize, in each state, train the neural network by following these steps:\n","\n","1. Choose an action using an epsilon-greedy policy, using the neural network to predict Q-values.\n","1. Take the action and record the next state and reward.\n","1. Calculate a target Q-value for the $(s,a)$ pair using the Bellman update.\n","1. Train the neural network to predict the target Q-value.\n","\n","Over many transitions, your neural network will learn to approximate the Q-values for every state-action pair. Using these Q-values, the epsilon-greedy policy can solve the `FrozenLake-v0` environment. This approach is called **online DQN** because the agent trains on the state transitions generated when it is running (online)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FGAcY2or2GgH"},"source":["## Implement Framework to Solve Frozen Lake"]},{"cell_type":"markdown","metadata":{"id":"E_eGZasYqhmM","colab_type":"text"},"source":["Define the functions you need to train your agent. Start by completing the following code cell to define a function that runs one training episode by repeating the steps described previously."]},{"cell_type":"code","metadata":{"id":"_H2MN-ItdvFJ","colab_type":"code","colab":{}},"source":["def collect_one_episode_and_train_model(env, model, epsilon, discount_factor):\n","  '''Runs one episode and trains the model on every state transition.\n","\n","  Runs one episode. On every state transition in the episode, collects the\n","  tuple s, a, r, s'. Then performs Bellman update on Q-values using the tuple\n","  and trains the agent to predict the updated Q-values.\n","\n","  Args:\n","    env: environment that the agent is learning.\n","    model: neural network used to predict Q-values of (state, action) pairs\n","    discount_factor: factor by which to reduce return from next state when\n","      updating Q-values using Bellman update.\n","  Returns:\n","    episode_length: number of states visited during episode\n","    episode_reward: total reward earned by agent during episode\n","    model: updated model after training during episode\n","  '''\n","  state = env.reset()\n","  episode_reward = 0\n","  done = False\n","  episode_length = 0\n","\n","  while not done:\n","    episode_length += 1\n","    # =========== Complete this section by replacing the \"...\" =================\n","    # q_values = ...\n","    # action = ...\n","    # state_new, reward, done, _ = ...\n","    # q_values[0, action] = ...\n","    # ==========================================================================\n","    model.fit(one_hot_encode_state(state), q_values, verbose=False)\n","    episode_reward += reward\n","    state = state_new\n","\n","  return(episode_length, episode_reward, model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4jT0xfzNqXii","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n","def collect_one_episode_and_train_model(env, model, epsilon, discount_factor):\n","  '''Runs one episode and trains the model on every state transition.\n","\n","  Runs one episode. On every state transition in the episode, collects the\n","  tuple s, a, r, s'. Then performs Bellman update on Q-values using the tuple\n","  and trains the agent to predict the updated Q-values.\n","\n","  Args:\n","    env: environment that the agent is learning.\n","    model: neural network used to predict Q-values of (state, action) pairs\n","    discount_factor: factor by which to reduce return from next state when\n","      updating Q-values using Bellman update.\n","  Returns:\n","    episode_length: number of states visited during episode\n","    episode_reward: total reward earned by agent during episode\n","    model: updated model after training during episode\n","  '''\n","  state = env.reset()\n","  episode_reward = 0\n","  done = False\n","  episode_length = 0\n","\n","  while not done:\n","    episode_length += 1\n","    q_values = model.predict(one_hot_encode_state(state))\n","    action = policy_eps_greedy(env, q_values, epsilon)\n","    state_new, reward, done, _ = env.step(action)\n","    q_values[0, action] = bellman_update(reward, discount_factor, model,\n","                                         state_new)\n","    model.fit(one_hot_encode_state(state), q_values, verbose=False)\n","    episode_reward += reward\n","    state = state_new\n","\n","  return(episode_length, episode_reward, model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uouvJy-TrIbD","colab_type":"text"},"source":["Define a function to test the agent's performance for a given success threshold. You will use this function to detect whether the agent has solved the enviroment."]},{"cell_type":"code","metadata":{"id":"OHLJZ4OIfw3B","colab_type":"code","cellView":"both","colab":{}},"source":["def check_success(episode, reward_history, length_history, epsilon,\n","                 success_percent_threshold):\n","  '''Returns 1 if agent has crossed success threshold.\n","\n","  For a fixed number of episodes, calculates and prints metrics summarizing\n","  agent's training over those episodes. Then checks and returns 1 if agent\n","  has crossed the defined success threshold. Otherwise, returns 0.\n","\n","  Args:\n","    episode: episode number of agent's training\n","    reward_history: list containing rewards for every episode\n","    length_history: list containing length of every episode, where length is\n","      the number of states visited during the episode\n","    epsilon: current value of epsilon\n","    success_percent_threshold: percent of episodes that the agent must solve\n","      to prove that it is successfully learning the environment\n","  Returns:\n","    1 if the agent crossed the success threshold, 0 otherwise.\n","  '''\n","  if((episode+1) % CHECK_SUCCESS_INTERVAL == 0):\n","    # Check the success % in the last 100 episodes\n","    success_percent = np.sum(reward_history[-100:-1])\n","    length_avg = int(np.sum(length_history[-100:-1])/100.0)\n","    print(\"Episode: \" + f\"{episode:0>4d}\" + \\\n","          \", Success: \" + f\"{success_percent:2.0f}\" + \"%\" + \\\n","          \", Avg length: \" + f\"{length_avg:0>2d}\" + \\\n","          \", Epsilon: \" + f\"{epsilon:.2f}\")\n","    if(success_percent > success_percent_threshold):\n","      print(\"Agent crossed success threshold of \" + \\\n","            str(success_percent_threshold) + '%.')\n","      return(1)\n","  return(0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rzKmi11OzaLF","colab_type":"text"},"source":["Using the functions `collect_one_episode_and_train_model` and `check_success`, define a function to train the agent until the agent crosses the success threshold:"]},{"cell_type":"code","metadata":{"id":"Scp6WgDEziQg","colab_type":"code","colab":{}},"source":["#### Plotting functions ####\n","def visualize_training(reward_history):\n","  plt.plot(range(len(reward_history)), reward_history)\n","  plt.xlabel('Episodes')\n","  plt.ylabel('Reward')\n","  plt.title('Reward during Training')\n","  plt.show()\n","\n","#### Training function ####\n","def train_agent(env, model, episodes, epsilon, discount_factor, eps_decay,\n","               success_percent_threshold):\n","  '''Trains the agent by running episodes while checking for successful\n","     learning.\n","  Args:\n","    env: environment to train the agent on\n","    model: neural network representing agent used to learn Q-values of\n","      environment\n","    epsilon: starting value of epsilon\n","    discount_factor: factor by which to reduce return from next state when\n","      updating Q-values using Bellman update.\n","    eps_decay: factor to reduce value of epsilon by, on every episode\n","    episodes: number of episodes to train agent for\n","    learning_rate: learning rate used by model\n","  '''\n","  length_history = []     # Record agent's episode length\n","  reward_history = []     # Record agent's episode reward\n","  timeStart = time.time() # Track training time\n","\n","  for episode in range(episodes):\n","    episode_length, episode_reward, model = \\\n","      collect_one_episode_and_train_model(env, model, epsilon, discount_factor)\n","    length_history.append(episode_length)\n","    reward_history.append(episode_reward)\n","    if epsilon > EPSILON_MIN:\n","      epsilon *= eps_decay\n","    if(check_success(episode, reward_history, length_history, epsilon,\n","                 success_percent_threshold)):\n","      break\n","\n","  timeEnd = time.time()\n","  print(\"Training time (min): \" + f'{(timeEnd - timeStart)/60:.2f}')\n","  visualize_training(reward_history)\n","  env.close() # Close environment"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eKk1kXThRMpX","colab_type":"text"},"source":["## Train Agent to Solve Frozen Lake"]},{"cell_type":"markdown","metadata":{"id":"-bQtaEcKv6nV","colab_type":"text"},"source":["Run the code below to solve `FrozenLake-v0` using DQN. To solve Frozen Lake, you must play with hyperparameter values. In doing so, your goal is to develop intuition for how hyperparameters interact to affect the training outcome. \n","\n","Consider the following advice on adjusting hyperparameter values:\n","\n","* Journey length begins increasing before success rate. Hence, journey length is a leading indicator of improvement. Further, journey length is a more stable metric than success percent.\n","* Aim to prioritize quick experimentation. For example, stop training if journey length doesn't begin increasing within 2000 episodes and try again.\n","* The agent should solve the environment in <5000 episodes.\n","* The output plot should show the incidence of successful episodes increasing.\n","* Frozen Lake is slightly more complex than NChain. Adjust `learning_rate` accordingly. \n","* The reward from the final state must propagate back to the initial state's Q-values. The higher the `discount_factor`, the greater the fraction of the reward that propagates back. Hence, keep `discount_factor` high.\n","\n","For the solution, expand the following section."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TxQr2GAMGD8B","cellView":"both","colab":{}},"source":["##### SETUP #####\n","episodes = 5000\n","epsilon = 1.0\n","eps_decay = 0.99\n","learning_rate = 0.01\n","discount_factor = 0.999\n","success_percent_threshold = 20 # in percent, so 60 = 60%\n","\n","model = define_model(learning_rate)\n","\n","#### TRAINING #####\n","train_agent(env, model, episodes, epsilon, discount_factor, eps_decay,\n","               success_percent_threshold)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3_HCbNWf3RPP"},"source":["## Solution (expand to view code)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nZOrUw8d3R5K"},"source":["The following code typically crosses a success rate of 20% in <2000 episodes. In the next cell, you'll visualize the trained agent solving the environment."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B0-Utan63TuQ","colab":{}},"source":["##### SETUP #####\n","episodes = 5000\n","epsilon = 1.0\n","eps_decay = 0.999\n","learning_rate = 0.2\n","discount_factor = 0.99\n","success_percent_threshold = 60 # in percent, so 60 = 60%\n","\n","model = define_model(learning_rate)\n","\n","#### TRAINING #####\n","train_agent(env, model, episodes, epsilon, discount_factor, eps_decay,\n","               success_percent_threshold)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bn-1xnkeHU1q","colab_type":"text"},"source":["While Frozen Lake is a more complex environment than NChain, it is simple in comparison to environments such as Pong and Breakout. When solving more and more complex environments, apply the intuition gained from solving simpler environments by using the following guidelines:\n","\n","* The agent will take longer to find a successful path through random exploration. Therefore, epsilon must decay slower so that the agent explores for longer.\n","* The agent must use a deeper and wider neural network to approximate the increased complexity.\n","* The agent must train at a lower learning rate to adapt to the increased complexity.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BrYPnH2EGNRX"},"source":["## Visualize Performance of Trained Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hiKRXasI6_NR"},"source":["Seeing the metrics plots is one thing, but visualizing your agent succeed at retrieving the frisbee is another. Run the following code to visualize your agent solve `FrozenLake`:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LY9Gje9Q4N6J","cellView":"code","colab":{}},"source":["from IPython.display import clear_output # to clear output on every episode run\n","\n","state = env.reset()\n","done = False\n","while(not(done)):\n","  q_values = model.predict(np.identity(num_states)[state:state+1])\n","  action = np.argmax(q_values)\n","  state_new, reward, done,_ = env.step(action)\n","  state = state_new\n","  clear_output()\n","  env.render()\n","  time.sleep(0.5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iiv8fmR27U32"},"source":["## Conclusion and Next Steps"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_xqjRz8e7Vdt"},"source":["You learned how to combine neural networks with traditional reinforcement learning approaches to solve a simple environment.\n","\n","Move onto the next Colab: [Experience Replay and Target Networks](https://colab.research.google.com/drive/1DEv8FSjMvsgCDPlOGQrUFoJeAf67cFSo#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-experience-replay-and-target-networks).\n","\n","For reference, the sequence of course Colabs is as follows:\n","\n","1. [Problem Framing in Reinforcement Learning](https://colab.research.google.com/drive/1sUYro4ZyiHuuKfy6KXFSdWjNlb98ZROd#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-problem-framing)\n","1. [Q-learning Framework](https://colab.research.google.com/drive/1ZPsEEu30SH1BUqUSxNsz0xeXL2Aalqfa#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-q-learning)\n","1. [Tabular Q-Learning](https://colab.research.google.com/drive/1sX2kO_RA1DckhCwX25OqjUVBATmOLgs2#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-tabular-q-learning)\n","1. [Deep Q-Learning](https://colab.research.google.com/drive/1XnFxIE882ptpO83mcAz7Zg8PxijJOsUs#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-deep-q-learning)\n","1. [Experience Replay and Target Networks](https://colab.research.google.com/drive/1DEv8FSjMvsgCDPlOGQrUFoJeAf67cFSo#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-experience-replay-and-target-networks)"]}]}