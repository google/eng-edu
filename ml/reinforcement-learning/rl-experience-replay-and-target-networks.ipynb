{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rl-experience-replay-and-target-networks.ipynb","version":"0.3.2","provenance":[{"file_id":"/piper/depot/google3/engedu/ml/self_study/rl/rl-deep-q-learning.ipynb?workspaceId=karangill:mlcc::citc","timestamp":1552578826835}],"collapsed_sections":["9EjQt_o9Xf_L","FjNDm9Y1hEq0","AeTnl1wMo9Mb"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9EjQt_o9Xf_L"},"source":["## Copyright 2019 Google LLC."]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"oXzTW-CnXf_Q","colab":{}},"source":["#@title\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sSD6t3TxhEqB"},"source":["# 5. DQN Techniques: Experience Replay and Target Networks"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"a5rJcjDIhEqF"},"source":["In the previous Colab, you trained a neural network on the results of every state transition. This approach tends to produce unstable training. In this Colab, you'll understand why training becomes unstable. Then, you'll understand the following two techniques that stabilize Deep Q-Network (DQN) training:\n","\n","* experience replay\n","* target networks"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gr5-N3iLhEqI"},"source":["## Disadvantages of Online DQN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FSG6uwfxhEqL"},"source":["In the previous Colab, every state transition generated a tuple, and you trained your agent on that tuple. Training your agent only on tuples generated by live training is called **online DQN**. Let's see why online DQN training is unstable.\n","\n","The problem with online DQN is that training an agent on a trajectory of states means successive states are probably similar. Therefore, input data can be correlated. However, in general, input data to a model must be [independent and identically distributed (i.i.d)](https://developers.google.com/machine-learning/glossary/#iid). In practice, correlated input data means that the agent might not generalize well to other states, resulting in unstable training.\n","\n","In general, neural network training relies on the assumption that data is i.i.d. In this Colab, you'll apply a technique called experience replay to satisfy this assumption."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dBGCYpQOhEqM"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"urxZETIjO0c4","colab_type":"text"},"source":["Run the following cell to set up Google Analytics for the Colab. Data from  Google Analytics helps improve the Colab."]},{"cell_type":"code","metadata":{"id":"ngfeEbGgO3rN","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Set up Google Analytics for Colab\n","%reset -f\n","import uuid\n","client_id = uuid.uuid4()\n","\n","import requests\n","\n","# Bundle up reporting into a function.\n","def report_execution():\n","  requests.post('https://www.google-analytics.com/collect', \n","                data=('v=1'\n","                      '&tid=UA-48865479-3'\n","                      '&cid={}'\n","                      '&t=event'\n","                      '&ec=cell'            # <-- event type\n","                      '&ea=execute'         # <-- event action\n","                      '&el=rl-experience-replay-target-networks'   # <-- event label\n","                      '&ev=1'               # <-- event value\n","                      '&an=bundled'.format(client_id)))\n","\n","from IPython import get_ipython\n","get_ipython().events.register('post_execute', report_execution)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4Aqm_Rl8hEqN"},"source":["Run the following cell to import libraries and setup the environment:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7ikOt779hEqO","colab":{}},"source":["import gym\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from tensorflow import keras\n","from collections import deque\n","\n","CHECK_SUCCESS_INTERVAL = 100\n","EPSILON_MIN = 0.01\n","\n","env = gym.make('FrozenLake-v0')\n","\n","num_states = env.observation_space.n\n","num_actions = env.action_space.n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZSYEOGKQx9N7"},"source":["Run the following cell to define functions that perform the following tasks:\n","\n","* Define the neural network.\n","* Calculate the Bellman update.\n","* Select an action.\n","* Check the agent's training for success.\n","\n","These functions are identical to functions in the previous Colab."]},{"cell_type":"code","metadata":{"cellView":"form","colab_type":"code","id":"qPVskkQMhEqU","colab":{}},"source":["#@title Run cell to define model, Bellman update, select action, and check success (expand to view code)\n","\n","def one_hot_encode_state(state):\n","  \"\"\"Args:\n","     state: An integer representing the agent's state.\n","   Returns:\n","     A one-hot encoded vector of the input `state`.\n","  \"\"\"\n","  return np.identity(num_states)[state:state+1]\n","\n","def compute_bellman_target(discount_factor, reward, model, state_next):\n","  '''Returns the updated return calculation given the reward and next state.\n","  Args:\n","    discount_factor: factor by which to reduce return from next state when\n","    updating Q-values using Bellman update.\n","    reward: reward from state transition.\n","    model: model used to predict Q-values\n","    state_next: next state after state transition.\n","  Returns:\n","    updated Q-value using Bellman update\n","  '''\n","  return reward + discount_factor * \\\n","           np.max(model.predict(one_hot_encode_state(state_next)))\n","\n","def define_model(learning_rate):\n","  '''Returns a shallow neural net defined using tf.keras.\n","  Args:\n","    learning_rate: optimizer learning rate\n","  Returns:\n","    model: A shallow neural net defined using tf.keras input dimension equal to\n","    num_states and output dimension equal to num_actions.\n","  '''\n","  model = []\n","  model = keras.Sequential()\n","  model.add(keras.layers.Dense(input_dim = num_states,\n","                               units = num_actions,\n","                               activation = 'relu',\n","                               use_bias = False,\n","                               kernel_initializer = keras.initializers.RandomUniform(minval=1e-5, maxval=0.05)\n","                              )\n","           )\n","  model.compile(optimizer = keras.optimizers.SGD(lr = learning_rate),\n","                loss = 'mse'\n","               )\n","  print(\"======= Neural Network Summary =======\")\n","  print(model.summary())\n","  return model\n","\n","learning_rate = 0.2\n","model = define_model(learning_rate)\n","\n","def select_action(epsilon, state):\n","  \"\"\"Select action given Q-values using epsilon-greedy algorithm.\n","  Args:\n","    q_values: q_values for all possible actions from a state.\n","    epsilon: Current value of epsilon used to select action using epsilon-greedy\n","             algorithm.\n","  Returns:\n","    action: action to take from the state.\n","  \"\"\"\n","  if(np.random.rand() < epsilon):\n","    return np.random.randint(num_actions)\n","  q_values = model.predict(one_hot_encode_state(state))\n","  return np.argmax(q_values)\n","\n","def check_success(episode, epsilon, reward_history, length_history, time_history, success_percent_threshold):\n","  if((episode+1) % CHECK_SUCCESS_INTERVAL == 0):\n","    # Check the success % in the last 100 episodes\n","    success_percent = np.sum(reward_history[-100:-1])\n","    length_avg = int(np.sum(length_history[-100:-1])/100.0)\n","    time_avg = np.sum(time_history[-100:-1])/100.0\n","    print(\"Episode: \" + f\"{episode:0>4d}\" + \\\n","          \", Success: \" + f\"{success_percent:2.0f}\" + \"%\" + \\\n","          \", Avg length: \" + f\"{length_avg:0>2d}\" + \\\n","          \", Epsilon: \" + f\"{epsilon:.2f}\" + \\\n","          \", Avg time(s): \" + f\"{time_avg:.2f}\"\n","         )\n","    if(success_percent > success_percent_threshold):\n","      print(\"Agent crossed success threshold of \" + str(success_percent_threshold) + '%.')\n","      return(1)\n","  return(0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"l3WGBtCghEqX"},"source":["## Improving DQN with Experience Replay"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vXevAQ8AhEqY"},"source":["In online DQN, all previous tuples are discarded. Instead, previous tuples can be collected in a buffer. Now, the agent can replay those state transitions and train without needing to again experience those state transitions. This technique is called **experience replay**. The buffer storing the tuples is called a **replay buffer**.\n","\n","To implement experience replay, the agent follows these steps on every state transition:\n","\n","1. Save the transition's tuple $s, a, r, s'$ in the replay buffer.\n","1. Create a batch of tuples by sampling the buffer.\n","1. Train the neural network on the batch of tuples.\n","\n","The following schematic shows these steps:\n","\n","![A schematic showing the algorithm for implementing Experience Replay. The interaction of the agent with the environment generates tuples s, a, r, s'. The replay buffer stores these tuples. The agent samples a minibatch of tuples from the replay buffer and trains on this minibatch to update its policy. Then the agent interacts with the environment to generate more tuples. This loop between the agent, environment, replay buffer, and the training shows the algorithm for experience replay.](https://developers.google.com/machine-learning/reinforcement-learning/images/experience-replay.png)\n","<!--Source: https://docs.google.com/presentation/d/1b8KM93svquW-nd1B8MC9xvcwtIeoNTrMIR-FOFoEPu8/edit#slide=id.g286953c419_0_491 -->\n","\n","Implement the first step by creating a replay buffer using a Python [deque](https://docs.python.org/2/library/collections.html#collections.deque). Set the buffer size to 2000. You will understand the context for why the buffer size is 2000 later in this Colab."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ie75QxqmhEqZ","colab":{}},"source":["replay_buffer_size = 2000\n","replay_buffer = deque(maxlen = replay_buffer_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7z5txRIohEqe"},"source":["Collect transitions by using a random policy for a few episodes:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BolqGsWMhEqg","colab":{}},"source":["for episode in range(3):\n","  state = env.reset()\n","  done = False\n","  while not done:\n","    action = env.action_space.sample()\n","    state_next, reward, done, _ = env.step(action)\n","    replay_buffer.append((state, action, reward, state_next))\n","    state = state_next\n","\n","print(replay_buffer)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"B5axTL19hEqm"},"source":["Implement experience replay by defining a function to sample a batch from `replay_buffer` and train the agent on every tuple in the batch. Vectorize the code to train the model on the entire batch because training the model on a single tuple at a time is slow."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Evho_UrWhEqn","colab":{}},"source":["def sample_from_replay_buffer_and_train_model(replay_buffer, batch_size, model, discount_factor):\n","  '''Samples a batch from the buffer and trains the agent on the batch.\n","  \n","  Unpacks feature data from tuples of (state, action, reward, state_next).\n","  Encodes states as one-hot vectors and stacks these vectors into a matrix.\n","  Creates matrix of target Q-values. Uses both matrices to train model in one\n","  call for faster training.\n","  \n","  Args:\n","    replay_buffer: deque containing recorded tuples.\n","    batch_size: integer specifying training batch size.\n","    model: neural network representing agent.\n","    discount_factor: factor by which to reduce return from next state when\n","      updating Q-values using Bellman update.\n","  Returns:\n","    model: neural network trained on sampled batch.\n","  '''\n","  if(len(replay_buffer) > batch_size):\n","    batch = random.sample(replay_buffer, batch_size)\n","    # extract s, a, r, s' from tuples into vectors\n","    states = [item[0] for item in batch]\n","    actions = [item[1] for item in batch]\n","    rewards = [item[2] for item in batch]\n","    states_next = [item[3] for item in batch]\n","    # encode states as a matrix of one-hot vectors\n","    one_hot_encoded_states = np.empty(shape=(0,num_states))\n","    for state in states:\n","      one_hot_encoded_states = np.vstack((one_hot_encoded_states, one_hot_encode_state(state)))\n","    # predict Q-values and update predictions using Bellman update\n","    target_q_values = model.predict(one_hot_encoded_states) # TODO. This TODO is\n","            # a placeholder. You'll fill in code later, in Part 2 of this Colab.\n","    for i in range(len(states)):\n","      target_q_values[i, actions[i]] = compute_bellman_target(discount_factor, rewards[i], model, states_next[i])\n","    # now, you can run the following training step without a loop\n","    model.fit(one_hot_encoded_states, target_q_values, epochs = 1, verbose = False)\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rIkHo7g1hEqr"},"source":["Train the agent on the replay_buffer by running the following cell. Compare the best action for the first state before and after training."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YFVM6SgshEqs","colab":{}},"source":["batch_size = 8\n","discount_factor = 0.95\n","print(\"Q-values for state 0 -\")\n","print(\"Before training epoch:\", model.predict(one_hot_encode_state(0)))\n","\n","model = sample_from_replay_buffer_and_train_model(replay_buffer, batch_size, model, discount_factor)\n","\n","print(\"After training epoch: \", model.predict(one_hot_encode_state(0)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"peH6Q_85hEqv"},"source":["To summarize, on every state transition, the agent follows these steps:\n","\n","* Save the tuple from the state transition to the buffer.\n","* Samples a batch of tuples from replay_buffer and trains on the batch."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j5kIS0-ThEqw"},"source":["## Train and Evaluate DQN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j4mNuQS_hEqx"},"source":["Training with experience replay is slow. This slowness restricts how much you can explore the hyperparameter space. Follow these steps:\n","\n","1. From the previous Colab, copy the values for `eps_decay`, `discount_factor`, `episodes`, and `learning_rate`.\n","1. Set `replay_buffer_size` to an initial value. How can you estimate such a value?\n","1. `batch_size` is typically 16, 32, or 64. These are standard values in DQN. However, because FrozenLake is a simple environment, set `batch_size = 8` for faster training.\n","\n","Run the cell and experiment with hyperparameter values to train the agent. How does training with experience replay compare with training with online DQN? Expand the following section for a discussion."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dyqN5EQuhEqx","colab":{}},"source":["# Hyperparameters\n","epsilon = 1.0\n","eps_decay = 0.99\n","discount_factor = 0.999\n","episodes = 5000\n","learning_rate = 0.5\n","replay_buffer_size = 2000\n","batch_size = 8\n","# TODO. This TODO is a placeholder. You'll fill in code later,\n","# in Part 2 of this Colab.\n","\n","# Parameters & model\n","success_percent_threshold = 20 # in percent, so 60 = 60%\n","model = define_model(learning_rate)\n","# TODO. This TODO is a placeholder. You'll fill in code later,\n","# in Part 2 of this Colab.\n","replay_buffer = deque(maxlen = replay_buffer_size) # create new replay_buffer\n","\n","# Training metrics\n","length_history = []\n","reward_history = []\n","time_history = []\n","\n","# Test if parameter values are valid\n","assert eps_decay < 1.0 and eps_decay > 0.\n","assert success_percent_threshold > 9 # agent could reach 9% randomly\n","\n","print(\"======= Begin Training =======\")\n","for episode in range(episodes):\n","  state = env.reset()\n","  done = False\n","  episode_reward = 0\n","  episode_length = 0\n","  episode_time_start = time.time()\n","  while not done:\n","    episode_length += 1\n","    action = select_action(epsilon, state)\n","    state_next, reward, done, _ = env.step(action)\n","    replay_buffer.append((state, action, reward, state_next))\n","    model = sample_from_replay_buffer_and_train_model(\n","        replay_buffer, batch_size, model, discount_factor)\n","    # TODO. This TODO is a placeholder. You'll fill in code later,\n","    # in Part 2 of this Colab.\n","    episode_reward += reward\n","    state = state_next\n","\n","  # Decreasing epsilon here instead of inside sample_from_replay_buffer_and_train_model introduces\n","  # the possible edge condition that epsilon decreases before the\n","  # model starts training because the batch doesn't build up\n","  if epsilon > EPSILON_MIN:\n","    epsilon *= eps_decay\n","  length_history.append(episode_length)\n","  reward_history.append(episode_reward)\n","  time_history.append(time.time() - episode_time_start)\n","  \n","  if check_success(episode, epsilon, reward_history, length_history, time_history, success_percent_threshold):\n","    break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FjNDm9Y1hEq0"},"source":["### Discussion (expand to view)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C-GCqg5dhEq0"},"source":["Replay buffer size is a balance between weighing new trajectories vs. old trajectories. As your agent improves, new trajectories are probably more rewarding than old trajectories. However, using old trajectories makes your training more stable because your agent trains on more diverse data.\n","\n","Here, each episode has a length of about 7. The agent's initial success rate is about 2%. To ensure you have at least a few successful episodes in your memory, estimate a replay buffer containing about 200 episodes. 200 episodes are equivalent to about $200\\cdot7 = 1400$ state transitions. Any buffer size in that range is okay.\n","\n","Hyperparameter values that let the agent solve the environment are:\n","* `epsilon = 1.0`\n","* `eps_decay = 0.999`\n","* `discount_factor = 0.99`\n","* `episodes = 2000`\n","* `learning_rate = 0.2`\n","* `replay_buffer_size = 2000`\n","* `batch_size = 8`\n","\n","Observations from training:\n","* Training using experience replay is slower because you're training on a batch of tuples instead of a single tuple.\n","* When compared to the previous Colab, your agent solves the environment in approximately the same number of episodes. Possible causes are:\n","  * Frozen Lake is not a complex enough environment for experience replay to be advantageous.\n","  * The hyperparameters are not correctly optimized.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dBodPz41hEq1"},"source":["## Visualize Performance of Trained Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ekKAega-hEq2"},"source":["Seeing the metrics plots is one thing, but visualizing your agent succeed at retrieving the frisbee is another. Run the following code to visualize your agent solving `FrozenLake`."]},{"cell_type":"code","metadata":{"cellView":"code","colab_type":"code","id":"MArEIdchhEq4","colab":{}},"source":["from IPython.display import clear_output # to clear output on every episode run\n","\n","state = env.reset()\n","done = False\n","\n","epsilon = 0. # greedy policy\n","while(not(done)):\n","  action = select_action(epsilon, state)\n","  state_new, reward, done, _ = env.step(action)\n","  state = state_new\n","  clear_output()\n","  env.render()\n","  time.sleep(1.0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_qsTkgItvXkp"},"source":["## Advantages of Experience Replay\n","\n","The advantages of experience replay over online DQN are as follows:\n","\n","* Makes training more stable by training on batches of tuples instead of single tuples.\n","* Allows agent to generalize better by remembering past experience.\n","\n","However, experience replay does not fully address the instability in DQN. The next section describes another technique to stabilize DQN training—target networks."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FW72yiYshEq9"},"source":["## Target Networks"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0f9DYmrihEq-"},"source":["When you train the neural network using Bellman update, you're calculating the target Q-values for training using the neural network itself. Because the neural network trains using its own predictions, you create a feedback loop. Changes in the neural networks predictions can reinforce each other because the neural network tries to target its own fluctuating Q-values.\n","\n","The effect of fluctuations in target Q-values is magnified because the Q-values for a state depend on Q-values of successive states. Hence, changes in a state's Q-value can lead to changes in previous states' Q-values.\n","\n","To break the feedback loop, calculate target Q-values using a separate neural network, called a **target network**. To stabilize training, update your target network slowly to your main neural network. The simplest approach is to update your target network to the main network  on every $N$ steps. Alternatively, on every step, add a small correction to the target network's weights.\n","\n","The following schematic shows Q-learning with experience replay and target networks:\n","\n","![The following schematic shows the steps in the Q-learning algorithm when Q-learning is enhanced with these two techniques: experience replay and target networks. This schematic builds on the previous schematic for experience replay by adding an additional component. The new component is the target network. The agent uses the target network instead of the main network to predict Q-values. However, the agent continues to train only the main network. As the main network is trained, the agent slowly updates the weights of the target network from the main network. ](https://developers.google.com/machine-learning/reinforcement-learning/images/experience-replay-with-target-networks.png)"]},{"cell_type":"markdown","metadata":{"id":"ZShpwXVqxjcy","colab_type":"text"},"source":["Write a function to update the target network to the main neural network at a fixed interval of episodes by editing the following cell as indicated:"]},{"cell_type":"code","metadata":{"id":"CO6fkYh1r1g3","colab_type":"code","cellView":"both","colab":{}},"source":["def update_target_network(\n","    episode, update_target_network_interval, main_network, target_network):\n","  '''Updates the target network on every certain number of episodes by copying\n","  the model to the target network.\n","  \n","  Args:\n","    episode: integer representing episode number in agent's training.\n","    update_target_network_interval: integer  representing interval of episodes\n","      on which `target_network` is updated to `model`.\n","    main network: main neural network used to choose actions and train.\n","    target_network: neural network used to predict Q-values.\n","  Returns:\n","    the `target_network`, whether updated or not.\n","  '''\n","  if((episode+1) % update_target_network_interval == 0):\n","    target_network.set_weights(main_network.get_weights())\n","  return target_network"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nl3q3aSqxhBS","colab_type":"text"},"source":["The remaining steps consist of editing previously defined code to implement target networks.\n","\n","1. Add a hyperparameter to control the interval for the target network update:\n","  \n","  a. Go to this [line](#scrollTo=dyqN5EQuhEqx&line=14&uniqifier=1) marked by `TODO`.\n","  \n","  b. Set this hyperparameter.\n","\n","  > `update_target_network_interval = 10`\n","\n","1. Define the target network on this [line](#scrollTo=dyqN5EQuhEqx&line=14&uniqifier=1) marked by `TODO`. Insert this code:\n","\n","  > `target_network = define_model(learning_rate)`\n","\n","1. Update the target network:\n","\n","  a. Go to this [cell](#scrollTo=dyqN5EQuhEqx&line=1&uniqifier=1).\n","  \n","  b. Insert the call to `update_target_network` at the appropriate place.\n","\n","1. Predict Q-values by using `target network` instead of `model`:\n","\n","  a. Go to this [line](#scrollTo=Evho_UrWhEqn&line=30)  marked by `#TODO`. You are in the function definition for  `sample_from_replay_buffer_and_train_model`.\n","  \n","  b. Edit the line to predict target Q-values using `target_network` instead of `model`.\n","  \n","  c. Similarly, edit the following call to `compute_bellman_target` to use `target_network` instead of `model`.\n","  \n","  d. In the function's argument list, append the argument `target_network`. Accordingly, update the call to `sample_from_replay_buffer_and_train_model`.\n"," "]},{"cell_type":"markdown","metadata":{"id":"AeTnl1wMo9Mb","colab_type":"text"},"source":["### Solution Steps (expand to view)"]},{"cell_type":"markdown","metadata":{"id":"kMFRDq4zo-IM","colab_type":"text"},"source":["1. The call to calculate target Q-values in `sample_from_replay_buffer_and_train_model` should read:\n","  > `target_q_values = target_network.predict(one_hot_encoded_states)`\n","2. To update the target network, place the following call right after the call to train the model:\n","  > `target_network = update_target_network(episode, update_target_network_interval, model, target_network)`\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Z-R3XbfWhEq_"},"source":["## Conclusion and Next Steps"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7OI7weykhErA"},"source":["You learned how to stabilize neural network training by using the following techniques:\n","\n","* experience replay\n","* target networks\n","\n","These two techiques are building blocks in the success of modern deep Q-learning programs.\n","\n","Congratulations! You've completed the course Colabs. Return to the course [landing page](https://developers.google.com/machine-learning/reinforcement-learning/) to explore the Tensorflow library for Reinforcement Learning."]}]}