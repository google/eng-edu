{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rl-problem-framing.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["9EjQt_o9Xf_L","TDNMuuqSwLdt","jqaK-lQriNPO"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"85ANezc3CMqA","colab_type":"text"},"source":["# 1. Problem Framing in Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9EjQt_o9Xf_L"},"source":["## Copyright 2019 Google LLC."]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"oXzTW-CnXf_Q","colab":{}},"source":["#@title\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8dtAQeHRVYJz","colab_type":"text"},"source":["This Colab is part of the [Reinforcement Learning self-study course](https://developers.google.com/machine-learning/reinforcement-learning/). **Reinforcement Learning** (**RL**) is fundamentally a framework for a decision-making process. In this Colab, you'll do the following:\n","\n","* Learn basic RL terminology.\n","* Understand the state-action-reward framework.\n","* Learn to frame problems in RL."]},{"cell_type":"markdown","metadata":{"id":"HV5E0ctpCO6G","colab_type":"text"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"urxZETIjO0c4","colab_type":"text"},"source":["Run the following cell to set up Google Analytics for the Colab. We use Google Analytics to  improve these Colabs."]},{"cell_type":"code","metadata":{"id":"ngfeEbGgO3rN","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Setup Google Analytics for Colab\n","%reset -f\n","import uuid\n","client_id = uuid.uuid4()\n","\n","import requests\n","\n","# Bundle up reporting into a function.\n","def report_execution():\n","  requests.post('https://www.google-analytics.com/collect', \n","                data=('v=1'\n","                      '&tid=UA-48865479-3'\n","                      '&cid={}'\n","                      '&t=event'\n","                      '&ec=cell'                 # <-- event type\n","                      '&ea=execute'              # <-- event action\n","                      '&el=rl-problem-framing'   # <-- event label\n","                      '&ev=1'                    # <-- event value\n","                      '&an=bundled'.format(client_id)))\n","\n","from IPython import get_ipython\n","get_ipython().events.register('post_execute', report_execution)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hy8DtuKm7wme","colab_type":"text"},"source":["Run the following cell to import libraries. [**Gym**](https://gym.openai.com) is the standard RL library for specifying RL environments. You will use Gym throughout this course."]},{"cell_type":"code","metadata":{"id":"s1k8cAe3Ar4O","colab_type":"code","colab":{}},"source":["import gym\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nXX5_vTw7w0Q","colab_type":"text"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"X5mXs4Bk7xrR","colab_type":"text"},"source":["Suppose you've moved to a new neighborhood. You're looking for great restaurants in the neighborhood. At first, you explore randomly. Slowly, you begin  returning to the restaurants you like. After a while, you tend to stick to the restaurants you like. Through this process, you learned new favorite restaurants through reinforcement from repeated exploration. Reinforcement Learning formalizes this learning process.\n","\n","For a more concrete example, imagine you're playing Pac-Man. You need to eat dots and avoid ghosts. You can frame this scenario in RL as follows:\n","\n","* Pac-Man is the **agent**.\n","* The maze is the **environment**.\n","* Eating the dots returns **rewards**. Failure, such as being eaten by a ghost, is framed as a negative reward.\n","\n","If you assign a positive numeric reward to eating the dots, and a negative numeric reward to being eaten, then you can frame Pac-Man's objective as reward maximization.\n","\n","![Screenshot of the Pac-Man computer game.](https://www.google.com/logos/2010/pacman10-hp.png)\n","\n","To maximize reward, Pac-Man moves from cell to cell by going up, down, left, and right. In RL, these movements are called **actions**. On each action, the position of Pac-Man, the positions of the ghosts, and the available rewards change. Together, these positions and rewards are the environment's **state**. Therefore, the state changes on each action . An agent takes an action $a$ in a state $s$ and transitions to another state $s’$ while receiving a reward $r$.\n","\n","![A simple schematic shows the framework provided by Reinforcement Learning at a high level of abstraction. There are five boxes labeled. The first box, agent, is connected to the third box, environment, through the second box, action. The environment is connected back to the agent through two boxes: state and reward. ](https://developers.google.com/machine-learning/reinforcement-learning/images/rl-framing-outline.png)"]},{"cell_type":"markdown","metadata":{"id":"rMTGuBmdCcrX","colab_type":"text"},"source":["## Explore a Simple Environment"]},{"cell_type":"markdown","metadata":{"id":"6X7ZIdG49at9","colab_type":"text"},"source":["Understand the state-action-reward framework by exploring a simple Gym environment, called  `NChain-v0`.  The `NChain-v0` environment is a linear chain of states. When an agent takes an action in `NChain-v0`, the agent moves to a new state in the chain and receives a numeric reward. You will explore this chain of states by imitating such an agent. That is, you will map the environment by taking actions to move between states.\n","\n","Create the `NChain-v0` environment by running the following code:"]},{"cell_type":"code","metadata":{"id":"3AJQMa4PCcEV","colab_type":"code","colab":{}},"source":["env = gym.make('NChain-v0')\n","print('Resetting environment to starting state.')\n","state = env.reset()\n","print(\"Reset env to starting state '\" + str(state) + \"'.\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"irrM-XeEpdbq","colab_type":"text"},"source":["In the states of `NChain-v0`, the agent can take actions. Confirm that the environment allows two actions, labeled `0` and `1`:"]},{"cell_type":"code","metadata":{"id":"qFlVU9IgF5CW","colab_type":"code","colab":{}},"source":["print(\"Number of allowed actions: \" + str(env.action_space.n))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZ91BjHV8VoS","colab_type":"text"},"source":["The agent explores the environment by taking actions. After taking an action $a$ in a state $s$, the agent gets a reward $r$ and moves to a new state $s'$.\n","\n","Explore the environment by setting the action to `0` or `1` and running the code. Try to answer these questions:\n","\n","* How many states can you find?\n","* What reward values do you observe for state transitions?\n","* Do the outcomes of actions `0` and `1` differ?\n","\n","Do not spend more than a few minutes. Then view the discussion in the next section."]},{"cell_type":"code","metadata":{"id":"w7hEiOZy-N1V","colab_type":"code","cellView":"form","colab":{}},"source":["action = 0  #@param ; action is 0 or 1\n","state_next, reward, done, _ = env.step(action)\n","transition_tuple = \"In state = %d, taking action = %d returns reward = %d and state_next = %d\" % (state, action, reward, state_next)\n","if 'transitions' in locals():\n","  transitions.append(transition_tuple)\n","else:\n","  transitions = [transition_tuple]\n","for item in transitions:\n","  print(item)\n","state = state_next"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TDNMuuqSwLdt","colab_type":"text"},"source":["### Discussion (expand to view)"]},{"cell_type":"markdown","metadata":{"id":"FqolGggg-lzG","colab_type":"text"},"source":["The environment is a linear chain of states. Action `0` advances the agent along this chain with no reward. Action `1` returns the agent to the starting state $s_0$ with a small reward.\n","\n","However, at each action, the environment might return the result of the other action with a small probability. For example, action `0` might return the agent to the starting state, while `1` might advance the agent. The environmental probabilities that determine this next state $s'$ for a given action $a$ in a given state $s$ are defined by the **state-transition function** $P(s’|s,a)$.\n","\n","The outcome $s'$ for the state-action pair $s,a$ is captured in the tuple $(s,a,s')$. The environment returns a unique reward $r$ for every tuple $s,a,s'$. This reward is defined by the **reward function**  $R(s,a,s')$. Both these functions are defined by the environment independent of the agent.\n","\n","In practice, you never know the true reward function. Instead, this course represents the expected reward from taking action $a$ in state $s$ using the notation $R(s,a)$."]},{"cell_type":"markdown","metadata":{"id":"9JWcakUEc6-k","colab_type":"text"},"source":["## Map an Environment"]},{"cell_type":"markdown","metadata":{"id":"6dIxFDyH_CYX","colab_type":"text"},"source":["Now, let's try mapping out the environment by recording the agent's transitions. Each transition is characterized by the tuple $(s, a, r, s')$.  This sequence of tuples $(s, a, r, s’)$ is the agent’s **trajectory**.\n","\n","$$s_0 \\xrightarrow[r_0]{a_0} s_1 \\xrightarrow[r_1]{a_1} s_2 \\ldots \\xrightarrow[r_2]{a_{n-2}} s_{n-1}\\xrightarrow[r_{n-1}]{a_{n-1}} s_n\n","$$\n","\n","Reset the memory for recording transitions:"]},{"cell_type":"code","metadata":{"id":"F9-43uqwvWAr","colab_type":"code","colab":{}},"source":["transitions = []"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2KFJVKl_phmx","colab_type":"text"},"source":["Take an action by setting `action` and running the cell. The output shows the history of the tuple $(s,a,r,s')$. Can you find:\n","* All possible states.\n","* The possible state transitions and their probabilities.\n","* The rewards associated with (s,a) pairs."]},{"cell_type":"code","metadata":{"id":"TJbh75JUpiu0","colab_type":"code","cellView":"both","colab":{}},"source":["action = 0  # set `action` to `0` or `1`\n","\n","state_next, reward, _, _ = env.step(action)\n","transitions.append({ \"state\": state,\n","                     \"action\": action,\n","                     \"reward\": reward,\n","                     \"state_next\": state_next\n","                  })\n","for transition in transitions:\n","  print(transition)\n","state = state_next"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqaK-lQriNPO","colab_type":"text"},"source":["### Solution (expand to view)"]},{"cell_type":"markdown","metadata":{"id":"bAKPmiuY_fkn","colab_type":"text"},"source":["The environment is as follows, described in this [paper](https://ceit.aut.ac.ir/~shiry/lecture/machine-learning/papers/BRL-2000.pdf).\n","\n","<img alt=\"A schematic that shows the NChain environment. The schematic shows the states, possible actions, and results of taking those actions in the state. When an agent takes an action in a state, the agent moves to a new state and receives a reward. There are 5 states. The allowed actions from each state are labelled 0 and 1. Action 0 always leads to a reward of 0, except from state 4 where action 0 returns a reward of 10. Action 1 always returns a reward of 2.\" width=\"75%\" src=\"https://developers.google.com/machine-learning/reinforcement-learning/images/nchain-state-transitions.svg\"/>"]},{"cell_type":"markdown","metadata":{"id":"GBDuAk42dWpK","colab_type":"text"},"source":["Check your observations of the environment against these environment characteristics. Specifically, note the probabilistic nature of the environment.\n","\n","* Number of states: 5, labelled from `0` to `4`.\n","* State transition function:\n","  * `a=0` advances state from $s_n$ to $s_{n+1}$.\n","  * `a=1` returns to first state $s_0$.\n","  * The environment reverses the result of an action with a probability of $0.2$. That is, `a=0` will return $s_0$ and `a=1` will return $s_{n+1}$.\n","* Reward function:\n","  * `r=2` for returning to start.\n","  * `r=0` for advancing.\n","  * `r=10` for looping on state 4.\n","\n","Confirm these characteristics by querying the environment variables as follows:"]},{"cell_type":"code","metadata":{"id":"W00Ywce0_e2I","colab_type":"code","colab":{}},"source":["print(\"Number of states: \" + str(env.env.observation_space.n))\n","print(\"Reward on returning to starting state 0: \" + str(env.env.small))\n","print(\"Reward for looping on state 4: \" + str(env.env.large))\n","print(\"Probability that an action has the opposite effect: \" + str(env.env.slip))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3RYVqlNldpDK","colab_type":"text"},"source":["## Episodes"]},{"cell_type":"markdown","metadata":{"id":"gOqddH2s050F","colab_type":"text"},"source":["A trajectory from start to termination is called an **episode**. The trajectory terminates when the **termination condition** is met. The termination condition could be achieving a certain reward, taking too many actions without solving the environment, or reaching a terminal state.\n","\n","How many actions can you take before `NChain-v0` terminates? Run the following code to find out:"]},{"cell_type":"code","metadata":{"id":"Zvv0yaoR0nBt","colab_type":"code","colab":{}},"source":["counter = 0\n","state = env.reset()\n","done = False\n","while not done:\n","  state, _, done, _ = env.step(env.action_space.sample())\n","  counter +=1\n","print(\"Termination condition for number of actions = \" + str(counter))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gt3ei8Q4AhEJ","colab_type":"text"},"source":["Confirm that Gym sets the termination condition to a constant of 1000 actions:"]},{"cell_type":"code","metadata":{"id":"G2VxtXxWAiEi","colab_type":"code","colab":{}},"source":["env._max_episode_steps"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ct-LYY2WjE51","colab_type":"text"},"source":["## Conclusion and Next Steps"]},{"cell_type":"markdown","metadata":{"id":"0vvPAyvsjF99","colab_type":"text"},"source":["In this Colab, you learned:\n","\n","* Basic RL terminology:\n","    * agent\n","    * environment\n","    * state, action, and reward\n","    * state-transition function\n","    * reward function\n","    * episode\n","    * termination condition.\n","* How RL frames problems as state-action-reward frameworks.\n","* How RL environments are probabilistic.\n","\n","Move onto the next Colab: [Q-learning Framework](https://colab.research.google.com/drive/1ZPsEEu30SH1BUqUSxNsz0xeXL2Aalqfa#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-q-learning).\n","\n","For reference, the sequence of course Colabs is as follows:\n","\n","1. [Problem Framing in Reinforcement Learning](https://colab.research.google.com/drive/1sUYro4ZyiHuuKfy6KXFSdWjNlb98ZROd#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-problem-framing)\n","1. [Q-learning Framework](https://colab.research.google.com/drive/1ZPsEEu30SH1BUqUSxNsz0xeXL2Aalqfa#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-q-learning)\n","1. [Tabular Q-Learning](https://colab.research.google.com/drive/1sX2kO_RA1DckhCwX25OqjUVBATmOLgs2#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-tabular-q-learning)\n","1. [Deep Q-Learning](https://colab.research.google.com/drive/1XnFxIE882ptpO83mcAz7Zg8PxijJOsUs#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-deep-q-learning)\n","1. [Experience Replay and Target Networks](https://colab.research.google.com/drive/1DEv8FSjMvsgCDPlOGQrUFoJeAf67cFSo#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-experience-replay-and-target-networks)"]}]}