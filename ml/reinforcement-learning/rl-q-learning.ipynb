{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rl-q-learning.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["9EjQt_o9Xf_L","RoJTCqpHVa6b","STBKyPI0RRes","eOwGFQR3J9pv","7rImBJgsaIyj"]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"85ANezc3CMqA","colab_type":"text"},"source":["# 2. Q Learning Framework"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9EjQt_o9Xf_L"},"source":["## Copyright 2019 Google LLC."]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"oXzTW-CnXf_Q","colab":{}},"source":["#@title\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xzn4Yd9Igj8M","colab_type":"text"},"source":["This Colab is part of the [Reinforcement Learning course](https://developers.google.com/machine-learning/reinforcement-learning/). In the previous Colab, you learned to frame problems in reinforcement learning. In this Colab, you will learn about the RL decision-making process by applying the following concepts:\n","\n","* Markov Decision Process (MDP)\n","* Expected return and Q-value\n","* Bellman equation\n","\n","Lastly, you will use these concepts to solve the `NChain-v0` environment."]},{"cell_type":"markdown","metadata":{"id":"HV5E0ctpCO6G","colab_type":"text"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"urxZETIjO0c4","colab_type":"text"},"source":["Run the following cell to setup Google Analytics for the Colab. Data from  Google Analytics helps improve the Colab."]},{"cell_type":"code","metadata":{"id":"ngfeEbGgO3rN","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Set up Google Analytics for Colab\n","%reset -f\n","import uuid\n","client_id = uuid.uuid4()\n","\n","import requests\n","\n","# Bundle up reporting into a function.\n","def report_execution():\n","  requests.post('https://www.google-analytics.com/collect', \n","                data=('v=1'\n","                      '&tid=UA-48865479-3'\n","                      '&cid={}'\n","                      '&t=event'\n","                      '&ec=cell'            # <-- event type\n","                      '&ea=execute'         # <-- event action\n","                      '&el=rl-q-learning'   # <-- event label\n","                      '&ev=1'               # <-- event value\n","                      '&an=bundled'.format(client_id)))\n","\n","from IPython import get_ipython\n","get_ipython().events.register('post_execute', report_execution)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJSpW0e7FDfl","colab_type":"text"},"source":["Run the following cell to import libraries:"]},{"cell_type":"code","metadata":{"id":"s1k8cAe3Ar4O","colab_type":"code","colab":{}},"source":["import gym\n","import numpy as np\n","from IPython.display import clear_output\n","\n","np.set_printoptions(precision=2, suppress=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nXX5_vTw7w0Q","colab_type":"text"},"source":["## Markov Decision Process"]},{"cell_type":"markdown","metadata":{"id":"p6QcZBIGxA2G","colab_type":"text"},"source":["In the previous Colab, you explored the `NChain-v0` environment, visualized in the following graph:\n","\n","<img alt=\"A schematic that shows the NChain environment. The schematic shows the states, possible actions, and results of taking those actions in the state. When an agent takes an action in a state, the agent moves to a new state and receives a reward. There are 5 states. The allowed actions from each state are labelled 0 and 1. Action 0 always leads to a reward of 0, except from state 4 where action 0 returns a reward of 10. Action 1 always returns a reward of 2.\" width=\"75%\" src=\"https://developers.google.com/machine-learning/reinforcement-learning/images/nchain-state-transitions.svg\"/>\n","\n","Suppose you're in state 4. You know that action 0 returns a big reward with high probability.  This probability only depends on the current state, 4, and not on the previous sequence of states. This property, where possible state transitions are completely determined by the current state, is called the **Markov property**.\n","\n","From state 4, you decide to take action 0 to get the big reward. To make your decision, again, you only needed to know your current state. You didn't need knowledge of *how* you reached state 4. When an agent makes decisions to navigate a sequence of states under the Markov property, then the result is called a **Markov Decision Process (MDP)**.\n","\n","Recall that this sequence of states is called a trajectory, represented by $(s,a,r,s')$ tuples as follows:\n","\n","$$s_0 \\xrightarrow[r_0]{a_0} s_1 \\xrightarrow[r_1]{a_1} s_2 \\ldots \\xrightarrow[r_2]{a_{n-2}} s_{n-1}\\xrightarrow[r_{n-1}]{a_{n-1}} s_n\n","$$"]},{"cell_type":"markdown","metadata":{"id":"rMTGuBmdCcrX","colab_type":"text"},"source":["## Learning Rewards"]},{"cell_type":"markdown","metadata":{"id":"JHiKwUZV28Ep","colab_type":"text"},"source":["Under the Markov property, you know that the state-action pair s=4, a=0 will  probably return r=10, s'=4. That is, the next state and associated reward depend solely on the current state.\n","\n","Since rewards are specific to state-action pairs, you can track rewards for each state-action pair. First, for the `NChain-v0` environment, get the number of states and actions:"]},{"cell_type":"code","metadata":{"id":"3AJQMa4PCcEV","colab_type":"code","colab":{}},"source":["env = gym.make(\"NChain-v0\")\n","state = env.reset()\n","\n","num_states = env.observation_space.n\n","num_actions = env.action_space.n\n","\n","print(\"NChain-v0 has \" + str(num_states) + \" states and \" + str(num_actions) + \" actions.\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BrTrRlsKL_an","colab_type":"text"},"source":["Create a table to track rewards for each state-action transition. By default, initialize rewards to `0`.\n","\n","This table stores Q-values for every state-action pair in the following format:\n","\n","$$\n","\\text{states}\\left\\downarrow\\vphantom{\n","% the following matrix is invisible\n","% the \\vphantom uses the matrix to know its size\n","\\begin{bmatrix} \n","Q(s=0,a=0) & Q(s=0,a=1) \\\\\n","Q(s=1,a=0) & Q(s=1,a=1) \\\\\n","Q(s=2,a=0) & Q(s=2,a=1) \\\\\n","Q(s=3,a=0) & Q(s=3,a=1) \\\\\n","Q(s=4,a=0) & Q(s=4,a=1) \\\\\n","\\end{bmatrix}\n","}\n","\\right.\n","% This is the visible matrix\n","\\overset{\\xrightarrow{\\text{actions}}}\n","{\n","\\begin{bmatrix} \n","Q(s=0,a=0) & Q(s=0,a=1) \\\\\n","Q(s=1,a=0) & Q(s=1,a=1) \\\\\n","Q(s=2,a=0) & Q(s=2,a=1) \\\\\n","Q(s=3,a=0) & Q(s=3,a=1) \\\\\n","Q(s=4,a=0) & Q(s=4,a=1) \\\\\n","\\end{bmatrix}\n","}\n","$$"]},{"cell_type":"code","metadata":{"id":"_QLkc8o0L-b7","colab_type":"code","colab":{}},"source":["rewards = np.zeros([num_states, num_actions])\n","print(rewards)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IVOrtLcuV4_1","colab_type":"text"},"source":["Reset the environment. Then take an action (0 or 1) and assign the reward to the table. Try a few actions. For now, just observe how the rewards accumulate. Later, you will improve upon this method."]},{"cell_type":"code","metadata":{"id":"j2sowOKHWA-3","colab_type":"code","cellView":"form","colab":{}},"source":["action = 0 #@param\n","assert action == 0 or action == 1, \"Action must be 0 or 1.\"\n","\n","state_next, reward, _, _ = env.step(action)\n","rewards[state, action] = reward\n","transition = \"s=%d, a=%d, r=%d, s'=%d\" % (state, action, reward, state_next)\n","print(transition)\n","print(rewards)\n","state = state_next"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ntnd0V31WZw4","colab_type":"text"},"source":["Observe that the agent stays in the easy-to-reach states. Explore more state-action pairs by running the agent in a loop.\n","\n","To run the agent in a loop, you must automatically choose actions. The algorithm that chooses actions is called the **policy**. A simple policy algorithm is to choose actions randomly. Run the code below to define the **random policy**:"]},{"cell_type":"code","metadata":{"id":"DvmsWCaUW3Vo","colab_type":"code","colab":{}},"source":["def policy_random(num_actions):\n","  return np.random.randint(0,num_actions)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aJdTaDNdyJ8b","colab_type":"text"},"source":["Alternatively, sample a random action using the Gym library's built-in API:"]},{"cell_type":"code","metadata":{"id":"UtAyOd3nyUaS","colab_type":"code","colab":{}},"source":["def policy_random(env):\n","  return env.action_space.sample()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"STdrX2BGYo9L","colab_type":"text"},"source":["Run the following cell to run a full episode of the agent. For each transition tuple $(s,a,r,s')$, the agent assigns the reward `r` to the corresponding table cell `[s,a]`.\n","\n","Run the cell a few times and observe how the rewards table changes. Additionally, print the episode's total reward, called the **return**. Does the rewards table vary on each episode? Why? Expand the succeeding \"Solution\" section for the answer."]},{"cell_type":"code","metadata":{"id":"bt2vEydMYtQ8","colab_type":"code","colab":{}},"source":["state = env.reset()\n","done = False\n","rewards_table = np.zeros([num_states, num_actions])\n","episode_return = 0\n","\n","while not done: # episode terminates after 1000 actions\n","  action = policy_random(env)\n","  state_next, reward, done, _ = env.step(action)\n","  episode_return += reward\n","  rewards_table[state, action] = reward\n","  state = state_next\n","  print(rewards_table)\n","  clear_output(wait = True)\n","\n","print(rewards_table)\n","print(\"Return: \"+str(episode_return))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RoJTCqpHVa6b","colab_type":"text"},"source":["### Solution (expand to view)"]},{"cell_type":"markdown","metadata":{"id":"bL_q8Z0sVbue","colab_type":"text"},"source":["Yes, the rewards table varies on every episode, because the environment is probabilistic. Specifically, every $(s,a)$ transition has multiple possible outcomes $(r,s')$. The current rewards table only reflects the last recorded outcome because every outcome overwrites the previous outcome.\n","\n","For example, while (s=0,a=0) usually leads to (r=0, s'=1), sometimes it leads to (r=2, s'=0). Similarly, other state-action pairs that return r=0 with high probability have nonzero rewards. Therefore, because transitions are probabilistic, an agent cannot rely on a single transition to calculate reward. Instead, the agent must weight reward over multiple transitions."]},{"cell_type":"markdown","metadata":{"id":"ljLCwwHmMXnz","colab_type":"text"},"source":["## Learning Probabilistic Rewards"]},{"cell_type":"markdown","metadata":{"id":"i_VaRH9SMfTr","colab_type":"text"},"source":["Initialize the approximate reward $R(s,a)$ for each $(s,a)$ pair with some value, say $0$. Then, you can gradually refine $R(s,a)$ to approximate the probabilistic reward by adding a correction on each state transition.\n","\n","Programmatically, represent a correction to the approximated $R(s,a)$ by using the following update rule:\n","\n","$$R(s,a) \\gets R(s,a) + correction$$\n","\n","Weight each correction by a learning rate $\\alpha$. Now, you can repeat this correction for multiple state transitions. By weighting each correction to the reward, the final approximated reward reflects all the probabilistic state transitions experienced by the agent.\n","\n","$$R(s,a) \\gets R(s,a) + \\alpha \\cdot correction$$\n","\n","By definition, a correction is the difference between the measured reward $r_{s,a}$ and the expected reward $R(s,a)$:\n","\n","$$R(s,a) \\gets R(s,a) + \\alpha(r_{s,a} - R(s,a))$$\n","\n","Program this update rule in the following cell where indicated by `TODO`. Then run the code cell to generate the rewards table. On first glance, this rewards table looks promising. Do you think this rewards table could help the agent reach the big reward of 10? Expand the succeeding \"Solutions\" section for the answer."]},{"cell_type":"code","metadata":{"id":"rrPSs5d1OV7G","colab_type":"code","colab":{}},"source":["learning_rate = 0.1\n","\n","state = env.reset()\n","done = False\n","\n","rewards_table = np.zeros([num_states, num_actions])\n","episode_return = 0\n","\n","while(not done): # episode terminates after 1000 actions\n","  action = policy_random(env)\n","  state_new, reward, done, _ = env.step(action)\n","  episode_return += reward\n","  rewards_table[state,action] += # TODO: Code the update rule\n","  state = state_new\n","  print(rewards_table)\n","  clear_output(wait = True)\n","\n","print(rewards_table)\n","print(\"Return: \" + \"{:.2f}\".format(episode_return))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"STBKyPI0RRes","colab_type":"text"},"source":["### Solution (expand to view code)"]},{"cell_type":"markdown","metadata":{"id":"e1cSwz6kRSUU","colab_type":"text"},"source":["Run the following cell to implement the update rule and generate the rewards table.\n","\n","This rewards table will not help the agent reach the big reward. To reach the big reward, the agent must repeat a=0 till it reaches state 4. However, the rewards table tells an agent in state 0 that a=1 returns a larger reward than a=0. The rewards table does not account for the fact that taking a=0 brings the agent closer to the big reward. In other words, for each $(s,a)$ pair, the rewards table only tracks immediate reward instead of capturing the total possible reward.\n","\n","In the next section, you will learn how to estimate total reward."]},{"cell_type":"code","metadata":{"id":"s8513aVauCm5","colab_type":"code","colab":{}},"source":["learning_rate = 0.1\n","\n","state = env.reset()\n","done = False\n","\n","rewards_table = np.zeros([num_states, num_actions])\n","episode_return = 0\n","\n","while(not done): # episode terminates after 1000 actions\n","  action = policy_random(env)\n","  state_new, reward, done, _ = env.step(action)\n","  episode_return += reward\n","  rewards_table[state,action] += learning_rate * (reward - rewards_table[state, action])\n","  state = state_new\n","  print(rewards_table)\n","  clear_output(wait = True)\n","\n","print(rewards_table)\n","print(\"Return: \" + \"{:.2f}\".format(episode_return))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dbScBMpd0pS9","colab_type":"text"},"source":["## Q-Function and Q-Values"]},{"cell_type":"markdown","metadata":{"id":"nPXnij1om8cf","colab_type":"text"},"source":["Q-learning is the *fundamental* concept in this course. Ensure you understand this section.\n","\n","You must calculate the return, not the immediate reward. For a given $(s_0,a_0)$, the **return** is  the sum of all rewards until the episode terminates, denoted by $Q(s_0,a_0)$:\n","\n","$$ Q(s_0,a_0) = r_{s_0,a_0} + r_{s_1,a_1} + \\ldots + r_{s_{n-1}, a_{n-1}} $$\n","\n","Because the environment's rewards are probabilistic, $Q(s_0,a_0)$ is the *expected* return, called the **Q-function** or the **state-action** function.\n","\n","In the formula above, $Q(s_0,a_0)$ weights more distant rewards equally with less distant rewards. However, closer rewards are more desirable because they maximize reward faster. Therefore, account for the delayed nature of future rewards by introducing a discount factor $\\gamma$.\n","\n","$$ Q(s_0,a_0) = r_{s_0,a_0} +\\gamma r_{s_1,a_1} + \\gamma^2 r_{s_2,a_2} + \\ldots + \\gamma^{n-1} r_{s_{n-1}, a_{n-1}} $$\n","\n","Notice that the equation is recursive:\n","\n","$$ Q(s_0,a_0) = r_{s_0,a_0} + \\gamma Q(s_1,a_1) $$\n","\n","In this equation, you determine action $a_1$ using some policy, such as a random policy. Therefore, $Q(s_0,a_0)$ is the return from taking an action $a$ in a state $s$ and then following some policy that determines the future actions $a_1, a_2, \\ldots$.\n","\n","So far, your agent has chosen the action $a_1$ randomly. However, your agent should choose whatever action maximizes return. Modify the equation to choose the action $a_1$ that maximizes return:\n","\n","$$Q(s_0,a_0) = r_{s_0,a_0} +  \\gamma \\displaystyle \\max_{\\substack{a_1}} Q(s_1,a_1)$$\n","\n","Using this equation, an agent can update the approximated Q-values by using the update rule from [Learning Probabilistic Rewards](#scrollTo=ljLCwwHmMXnz) as follows:\n","\n","$$Q_{updated}(s_0,a_0) \\gets Q_{old}(s_0,a_0) + \\alpha \\cdot (Q_{calculated} - Q_{old}(s_0,a_0))$$\n","\n","Substituting for $Q_{calculated}$ with the expression for $Q(s_0,a_0)$:\n","\n","$$Q_{upd}(s_0,a_0) \\gets Q_{old}(s_0,a_0) + \\alpha\\underbrace{\n","  \\left(\n","      \\overbrace{\n","        \\underbrace{\n","        r_{s_0,a_0}\n","        }_{\\text{new reward}}\n","      + \\gamma \\displaystyle \\max_{\\substack{a_1}} Q(s_1,a_1)\n","      }^{\\text{calculated } Q(s_0,a_0)}\n","    - Q_{old}(s_0,a_0) \\right)\n","    }_{\\text{error gradient}}\n","$$\n","\n","This equation looks intimidating. Remember that the terms in the square brackets represent the correction, called the **error gradient**. In the error gradient, on each transition, the only new information is $r(s_0,a_0)$. The agent uses $r(s_0,a_0)$ to calculate the new Q-value $Q(s_0,a_0)$, then subtracts the old Q-value to get the error gradient, and finally weights the error gradient by a learning rate.\n","\n","The equation to update $Q$ is the famous Bellman equation as applied to RL. Take a moment to ensure you understand the equation.\n"]},{"cell_type":"markdown","metadata":{"id":"i4-asG1UInMV","colab_type":"text"},"source":["## Implement Bellman Equation"]},{"cell_type":"markdown","metadata":{"id":"r3vj3TDOIuY8","colab_type":"text"},"source":["Define a function that runs an episode and updates the Q-values by completing the missing Bellman update in the following cell. Your implementation must reproduce the last equation in the section above. The argument `learning_rate` is $\\alpha$ and the argument `discount_factor` is $\\gamma$.\n","\n","Check your implementation against the solution."]},{"cell_type":"code","metadata":{"id":"mT8ZGVlG8qN3","colab_type":"code","colab":{}},"source":["def run_training_episode(env, q_table, learning_rate, discount_factor):\n","  state = env.reset()\n","  done = False\n","  \n","  while(not done):\n","    action = policy_random(env)\n","    state_new, reward, done, _ = env.step(action)\n","    q_table[state,action] = q_table[state, action] + \\\n","         # TODO: Program Bellman update\n","         # HINT: max(Q(s_1,a_1)) = np.max(q_table[state_new,:])\n","    state = state_new\n","  \n","  return(q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1gsto50jFcX","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n","def run_training_episode(env, q_table, learning_rate, discount_factor):\n","  state = env.reset()\n","  done = False\n","  while(not done):\n","    action = policy_random(env)\n","    state_new, reward, done, _ = env.step(action)\n","    q_table[state,action] = q_table[state, action] + learning_rate*(\n","                                   reward + \\\n","                                   discount_factor * np.max(q_table[state_new,:]) - \\\n","                                   q_table[state, action]\n","                                 )\n","    state = state_new\n","  return(q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4vp8T3z09Azw","colab_type":"text"},"source":["## Train the Agent to Solve NChain"]},{"cell_type":"markdown","metadata":{"id":"BqPpQ9FwOkz7","colab_type":"text"},"source":["To train the agent, define a function that runs multiple episodes. For each episode, the function calls `run_training_episode` and prints the Q-table. This output shows the Q-table evolving over episodes. Fill out the call to `run_training_episode`."]},{"cell_type":"code","metadata":{"id":"GEIYezWiOlSr","colab_type":"code","colab":{}},"source":["def train_agent(env, episodes, learning_rate, discount_factor):\n","  q_table = np.zeros([num_states, num_actions])\n","  for episode in range(episodes):\n","    q_table = # TODO\n","    print(q_table)\n","    clear_output(wait = True)\n","  return(q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rQJwLys1iOL","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n","def train_agent(env, episodes, learning_rate, discount_factor):\n","  q_table = np.zeros([num_states, num_actions])\n","  for episode in range(episodes):\n","    q_table = run_training_episode(env, q_table, learning_rate, discount_factor)\n","    print(q_table)\n","    clear_output(wait = True)\n","  return(q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g0CBImP41s_P","colab_type":"text"},"source":["Recall that the Q-table stores Q-values for every state-action pair in the following format:\n","\n","$$\n","\\text{states}\\left\\downarrow\\vphantom{\n","% the following matrix is invisible\n","% the \\vphantom uses the matrix to know its size\n","\\begin{bmatrix} \n","Q(s=0,a=0) & Q(s=0,a=1) \\\\\n","Q(s=1,a=0) & Q(s=1,a=1) \\\\\n","Q(s=2,a=0) & Q(s=2,a=1) \\\\\n","Q(s=3,a=0) & Q(s=3,a=1) \\\\\n","Q(s=4,a=0) & Q(s=4,a=1) \\\\\n","\\end{bmatrix}\n","}\n","\\right.\n","% This is the visible matrix\n","\\overset{\\xrightarrow{\\text{actions}}}\n","{\n","\\begin{bmatrix} \n","Q(s=0,a=0) & Q(s=0,a=1) \\\\\n","Q(s=1,a=0) & Q(s=1,a=1) \\\\\n","Q(s=2,a=0) & Q(s=2,a=1) \\\\\n","Q(s=3,a=0) & Q(s=3,a=1) \\\\\n","Q(s=4,a=0) & Q(s=4,a=1) \\\\\n","\\end{bmatrix}\n","}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"4b3xFZMk8HbP","colab_type":"text"},"source":["Now, before training your agent, consider these questions with respect to the Q-table:\n","\n"," * Should $Q(s=0,a=0)$ be higher than $Q(s=0,a=1)$?\n"," * Should $Q(s=0,a=1)$ be higher than $Q(s=2,a=0)$?\n"," * How does the answer to these questions depend on $\\gamma$?\n"," \n","Remember that $Q$ measures the return, not the reward. Refer to the following graph for the environment when considering those questions:\n","\n","<img alt=\"A schematic that shows the NChain environment. The schematic shows the states, possible actions, and results of taking those actions in the state. The result is a new state and a reward. There are 5 states. The allowed actions from each state are labelled 0 and 1. Action 0 always leads to a reward of 0, except from state 4 where action 0 returns a reward of 10. Action 1 always returns a reward of 2.\" width=\"50%\" src=\"https://developers.google.com/machine-learning/reinforcement-learning/images/nchain-state-transitions.svg\"/>"]},{"cell_type":"markdown","metadata":{"id":"eOwGFQR3J9pv","colab_type":"text"},"source":["### Answers (expand to view)"]},{"cell_type":"markdown","metadata":{"id":"8dWCtIplJ45v","colab_type":"text"},"source":[" * Should $Q(s=0,a=0)$ be higher than $Q(s=0,a=1)$?\n","    *  Yes, because $s=0,a=0$ leads to the big reward, and therefore a higher return. Q-values measure return, not reward.\n"," * Should $Q(s=0,a=1)$ be higher than $Q(s=2,a=0)$?\n","   * No, because $(s=0,a=1)$ prevents the agent from reaching the big reward, while $(s=2,a=0)$ brings the agent closer to the big reward. Therefore, $(s=2,a=0)$ leads to a higher return ( Q-value).\n"," * How does the answer to these questions depend on $\\gamma$?\n","   * $\\gamma$ determines the increase in Q-value of a state-action pair from later rewards. A higher $\\gamma$ increases the propagation of rewards to Q-values of preceding state-action pairs. Hence, the previous two answers hold for a high $\\gamma$ but not for a low $\\gamma$."]},{"cell_type":"markdown","metadata":{"id":"K3aJ2DYDKB7m","colab_type":"text"},"source":["### Run Training"]},{"cell_type":"markdown","metadata":{"id":"6EE-fC-RJ72O","colab_type":"text"},"source":["Run the following cell to calculate Q-values over multiple episodes. Adjust `learning_rate`, `discount_factor`, and `episodes` to return Q-values that match your expectations. For the solution, expand the following section."]},{"cell_type":"code","metadata":{"id":"syPD6XkzIrgm","colab_type":"code","colab":{}},"source":["discount_factor = 0.8   #@param\n","learning_rate = 0.01   #@param\n","episodes = 5   #@param\n","\n","q_table = train_agent(env, episodes, learning_rate, discount_factor)\n","print(q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7rImBJgsaIyj","colab_type":"text"},"source":["### Solution (expand to view)"]},{"cell_type":"markdown","metadata":{"id":"KE2mb7OyaObC","colab_type":"text"},"source":["Run the following code to solve the environment. The rewards table shows that the best action is always 0. See discussion in the following cell."]},{"cell_type":"code","metadata":{"id":"PJb6l0UsFnGx","colab_type":"code","colab":{}},"source":["discount_factor = 0.95   #@param\n","learning_rate = 0.1    #@param\n","episodes = 10   #@param\n","\n","q_table = train_agent(env, episodes, learning_rate, discount_factor)\n","print(q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Pv4q3lk7yxy","colab_type":"text"},"source":["The solution above uses `learning_rate = 0.1`. In RL in general, `0.1` is a high value for `learning_rate`. RL agents typically learn environments using a much lower value of `learning_rate`. However, your agent can learn `NChain` using a high value of `learning_rate` because `NChain` is a very simple environment.\n","\n","In fact, your agent can learn `NChain` in one episode using a learning rate of `0.5`. Try it."]},{"cell_type":"markdown","metadata":{"id":"CtdeOo75AU9m","colab_type":"text"},"source":["## Test Your Trained Agent"]},{"cell_type":"markdown","metadata":{"id":"CnA5cCm3AWVG","colab_type":"text"},"source":["You've completed the hard work of training your agent. Now let's compare your trained agent to an agent choosing random actions. Your trained agent can maximize reward from every state transition by following a policy that always chooses the action with the highest Q-value. Such a policy is called a **greedy policy**.\n","\n","Define a function that runs your agent by following either a random policy or a greedy policy:"]},{"cell_type":"code","metadata":{"id":"rqZcFH5sA7Pv","colab_type":"code","colab":{}},"source":["def run_episode(env, q_table, policy_flag):\n","  state = env.reset()\n","  done = False\n","  episode_return = 0\n","  while(not done):\n","    if(policy_flag=='random'):\n","      action = env.action_space.sample()\n","    elif(policy_flag=='greedy'):\n","      action = np.argmax(q_table[state,:])\n","    else:\n","      raise Exception(\"Error: Policy flag must be 'random' or 'greedy'.\")\n","    state_new, reward, done, _ = env.step(action)\n","    episode_return += reward\n","    state = state_new\n","  return(episode_return)\n","\n","def run_agent(env, episodes, q_table, policy_flag):\n","  reward_avg = 0.0\n","  for episode in range(episodes):\n","    reward_avg += run_episode(env, q_table, policy_flag)\n","  return(reward_avg/episodes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xLxcCPIAC2-T","colab_type":"text"},"source":["Compare the average reward found by random and greedy agents over 10 episodes."]},{"cell_type":"code","metadata":{"id":"Tl5FS8clCu7b","colab_type":"code","colab":{}},"source":["episodes = 10\n","print(\"Average returns over \" + str(episodes) + \" episodes by -\")\n","print(\"Trained agent: \" + \\\n","     str(run_agent(env, episodes, q_table, 'greedy')))\n","print(\"Random agent: \" + \\\n","     str(run_agent(env, episodes, q_table, 'random')))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ifPyEZvhFIdf","colab_type":"text"},"source":["The trained agent is superior to the random agent."]},{"cell_type":"markdown","metadata":{"id":"xcdqtOz2esPD","colab_type":"text"},"source":["## Contrasting RL with Supervised Learning"]},{"cell_type":"markdown","metadata":{"id":"QAg3odJzev4C","colab_type":"text"},"source":["You might notice that the equations for gradient descent and the Bellman equation look similar. The equation for gradient descent is:\n","$$a_{n+1} = a_n - \\gamma \\nabla F(a_n)$$\n","\n","The difference is in the gradient calculation. In supervised learning, the loss is the gradient of the loss function, which is the delta between the predicted and actual values. In RL, the loss is the gradient of the delta between the newly estimated return and the old estimate of the return."]},{"cell_type":"markdown","metadata":{"id":"4lXbMW2IaMCh","colab_type":"text"},"source":["## Conclusion and Next Steps"]},{"cell_type":"markdown","metadata":{"id":"DvrSb62jAzvx","colab_type":"text"},"source":["You learned how to solve a simple environment using Q-learning. In the next Colab, you'll learn how to solve a more complex environment using Q-learning.\n","\n","Move onto the next Colab: [Tabular Q-Learning](https://colab.research.google.com/drive/1sX2kO_RA1DckhCwX25OqjUVBATmOLgs2#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-tabular-q-learning).\n","\n","For reference, the sequence of course Colabs is as follows:\n","\n","1. [Problem Framing in Reinforcement Learning](https://colab.research.google.com/drive/1sUYro4ZyiHuuKfy6KXFSdWjNlb98ZROd#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-problem-framing)\n","1. [Q-learning Framework](https://colab.research.google.com/drive/1ZPsEEu30SH1BUqUSxNsz0xeXL2Aalqfa#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-q-learning)\n","1. [Tabular Q-Learning](https://colab.research.google.com/drive/1sX2kO_RA1DckhCwX25OqjUVBATmOLgs2#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-tabular-q-learning)\n","1. [Deep Q-Learning](https://colab.research.google.com/drive/1XnFxIE882ptpO83mcAz7Zg8PxijJOsUs#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-deep-q-learning)\n","1. [Experience Replay and Target Networks](https://colab.research.google.com/drive/1DEv8FSjMvsgCDPlOGQrUFoJeAf67cFSo#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-experience-replay-and-target-networks)"]}]}