{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rl-tabular-q-learning.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["9EjQt_o9Xf_L","Ag5jg3gVW5kT"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9sEgulqSOWOs","colab_type":"text"},"source":["# 3. Tabular Q-Learning with Policy Algorithms\n","\n","In the previous Colab, you used Q-learning to build a table of rewards. In this Colab, you'll learn to exploit the table of Q-values. **Tabular Q-Learning** refers to implementing Q-learning by using a table of Q-values."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9EjQt_o9Xf_L"},"source":["## Copyright 2019 Google LLC."]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"oXzTW-CnXf_Q","colab":{}},"source":["#@title\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bbY8Lfsyd17L","colab_type":"text"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"urxZETIjO0c4","colab_type":"text"},"source":["Run the following cell to setup Google Analytics for the Colab. Data from  Google Analytics helps improve the Colab."]},{"cell_type":"code","metadata":{"id":"ngfeEbGgO3rN","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Set up Google Analytics for Colab\n","%reset -f\n","import uuid\n","client_id = uuid.uuid4()\n","\n","import requests\n","\n","# Bundle up reporting into a function.\n","def report_execution():\n","  requests.post('https://www.google-analytics.com/collect', \n","                data=('v=1'\n","                      '&tid=UA-48865479-3'\n","                      '&cid={}'\n","                      '&t=event'\n","                      '&ec=cell'            # <-- event type\n","                      '&ea=execute'         # <-- event action\n","                      '&el=rl-tabular-q-learning'   # <-- event label\n","                      '&ev=1'               # <-- event value\n","                      '&an=bundled'.format(client_id)))\n","\n","from IPython import get_ipython\n","get_ipython().events.register('post_execute', report_execution)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sqq7F_xlP5IJ","colab_type":"text"},"source":["Run the following cell to import libraries and create the environment. The environment is called `FrozenLake-v0`. You will explore the environment in the next section."]},{"cell_type":"code","metadata":{"id":"dA0NNe-1d2nz","colab_type":"code","colab":{}},"source":["import numpy as np\n","import gym\n","import time\n","import math\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output # to clear output on every episode run\n","\n","np.set_printoptions(precision=5, suppress=True)\n","\n","env = gym.make('FrozenLake-v0')\n","state = env.reset()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"be7FWKbsstLC","colab_type":"text"},"source":["## Understand the Environment"]},{"cell_type":"markdown","metadata":{"id":"ghr8rcgVsvzj","colab_type":"text"},"source":["The [`FrozenLake-v0`](https://gym.openai.com/envs/FrozenLake-v0) environment is more complex than `NChain-v0`. You must cross a frozen lake to retrieve a frisbee. The lake is a 4x4 grid:\n","    \n","          SFFF\n","          FHFH\n","          FFFH\n","          HFFG\n","\n","      S : starting point, safe\n","      F : frozen surface, safe\n","      H : hole, fall to your doom\n","      G : goal, where the frisbee is located\n","\n","You start at the top-left cell. The frisbee is in the bottom-right cell. You receive a reward of 1 upon reaching the frisbee, and 0 for all other transitions. The episode ends on reaching the frisbee or falling into a hole. Warning: Because the ice is slippery, the result of taking an action (up, down, left, right) is probabilistic.\n","\n","First, record the number of possible states and actions:"]},{"cell_type":"code","metadata":{"id":"3M14eayUSdRd","colab_type":"code","colab":{}},"source":["num_states = env.observation_space.n\n","print(\"Number of states: \" + str(num_states))\n","num_actions = env.action_space.n\n","print(\"Number of actions: \" + str(num_actions))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gf1hvIwmSdlB","colab_type":"text"},"source":["Run the following cell a few times to observe an episode and understand the environment. The output shows an agent stepping through a full episode using a random policy. The action taken is in parentheses. Notice how the agent's state transitions often do not correspond to the action taken. The environment is very probabilistic."]},{"cell_type":"code","metadata":{"id":"FBkQ-0vmeiVs","colab_type":"code","colab":{}},"source":["done = False\n","state = env.reset()\n","env.render()\n","\n","while not done:\n","  clear_output(True)\n","  state_next, reward, done, _ = env.step(env.action_space.sample())\n","  env.render()\n","  time.sleep(1.0)\n","  state = state_next"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SKX4OEKzPVVE","colab_type":"text"},"source":["## Random & Greedy Policies\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q4HqNjdyYuAL","colab_type":"text"},"source":["Build the Q-values table for the environment by using a random policy to explore the enviroment. First, define the random policy:"]},{"cell_type":"code","metadata":{"id":"3DgmuU-RfIXl","colab_type":"code","colab":{}},"source":["def policy_random():\n","  return env.action_space.sample()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U49xojTUb47j","colab_type":"text"},"source":["Define a function to update the Q-table using the Bellman equation: \n","\n","$$Q(s,a) \\gets Q(s,a) + \\alpha\n","  \\left[r(s,a)\n","      + \\gamma \\displaystyle\\max_{\\substack{a_1}} Q(s_1,a_1)\n","    - Q(s,a) \\right]\n","$$\n","\n","Complete the function's code as indicated:"]},{"cell_type":"code","metadata":{"id":"5e9sc669b-FZ","colab_type":"code","colab":{}},"source":["def bellman_update(q_table, learning_rate, discount_factor, reward, state, state_next, action):\n","  q_table[state,action] = q_table[state, action] + # TODO: bellman update\n","  return q_table"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYMFsWl1yzwr","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n","def bellman_update(q_table, learning_rate, discount_factor, reward, state, state_next, action):\n","  q_table[state,action] = q_table[state, action] + \\\n","                            learning_rate*(reward + \\\n","                                   discount_factor*np.max(q_table[state_next,:]) - \\\n","                                   q_table[state, action]\n","                                 )\n","  return q_table"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"73y3y3Z8qzfD","colab_type":"text"},"source":["Define a function to run an episode using the random policy and the Bellman update. Complete the function as indicated:"]},{"cell_type":"code","metadata":{"id":"PyqW0tCLq2KX","colab_type":"code","colab":{}},"source":["def run_random_episode(env, q_table, learning_rate, discount_factor):\n","  state = env.reset()\n","  done = False\n","  \n","  while(not done):\n","    action =  # TODO\n","    state_next, reward, done, _ =  # TODO\n","    q_table = # TODO\n","    state =   # TODO\n","  \n","  return(q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mcD6ZJWOOKYE","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n","def run_random_episode(env, q_table, learning_rate, discount_factor):\n","  state = env.reset()\n","  done = False\n","  \n","  while(not done):\n","    action = policy_random()\n","    state_next, reward, done, _ = env.step(action)\n","    q_table = bellman_update(q_table, learning_rate, discount_factor, reward, state, state_next, action)\n","    state = state_next\n","  \n","  return(q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kOaKc9VgNkXz","colab_type":"text"},"source":["Create the Q-table by running the random policy for a few thousand episodes. Do not try to read the resulting Q-table closely because the table is complex. Instead, note the following characteristics of the Q-table:\n","\n","* Q-values remain 0 for the terminal states (the holes and the goal state) because no transitions occur from terminal states.\n","* Q-value for the final state-action pair that precedes the goal, (s=14, a=2), is set to 100 by the code. The code normalizes all other Q-values.\n","* The higher the Q-value of a state-action pair, the closer that state-action pair is to the goal."]},{"cell_type":"code","metadata":{"id":"OC47KVJRODME","colab_type":"code","colab":{}},"source":["discount_factor = 0.9 # typical value\n","learning_rate = 0.5 # typical value for FrozenLake is 0.1 to 0.8\n","episodes = 2000 # typically varies from 5000 to 15000\n","\n","q_table = np.zeros([num_states, num_actions])\n","\n","# Normalize Q-values for readability\n","def normalize_q_values(q_table):\n","  q_table_max = np.max(q_table)\n","  if q_table_max > 0.0:  # if agent never succeeds, then max(q_table) = 0\n","    q_table = (q_table/q_table_max)*100.0\n","  return q_table\n","\n","for episode in range(episodes):\n","  q_table = run_random_episode(env, q_table, learning_rate, discount_factor)\n","print(normalize_q_values(q_table))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LUksSdWvN3u6","colab_type":"text"},"source":["Using these Q-values, you can exploit the environment by following the path of maximum Q. A policy that exploits the environment by maximizing Q is called a **greedy policy**.\n","\n","Define a policy function that uses Q-values to greedily choose an action:"]},{"cell_type":"code","metadata":{"id":"Sv4BQkdVNjgk","colab_type":"code","colab":{}},"source":["def policy_greedy(q_table, state):\n","  return np.argmax(q_table[state,:])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hr6LzTDKOK9z","colab_type":"text"},"source":["Run the following code to follow a greedy policy and find the successful percentage of episodes. The greedy policy has a low success percent. Therefore, the Q-values must be inaccurate."]},{"cell_type":"code","metadata":{"id":"Wddtd_56yVxr","colab_type":"code","colab":{}},"source":["total_reward = 0.0\n","\n","num = 10000\n","for episode in range(num):\n","  state = env.reset()\n","  done = False\n","  while(not done):\n","    action = policy_greedy(q_table, state)\n","    state, reward, done,_ = env.step(action)\n","  total_reward += reward\n","\n","print(\"Percentage of successful episodes: \" + \\\n","      str(total_reward / num * 100) + \"%.\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4l03Tn3HOSsy","colab_type":"text"},"source":["## Balancing Exploitation with Exploration"]},{"cell_type":"markdown","metadata":{"id":"sHe3uBieOUPe","colab_type":"text"},"source":["Why are the Q-values inaccurate after thousands of episodes of exploration? At the beginning, all Q-values are 0. As the agent runs successive episodes with a random policy, the agent finds rewarding paths. However, the random policy doesn't let the agent change behavior to exploit the most rewarding paths. Therefore, the agent does not explore the environment enough to find the paths that lead to maximum return.\n","\n","To improve upon a random policy, you can use an **epsilon greedy** (e-greedy) policy instead. This policy takes a random action with epsilon probability and a greedy action otherwise. The value of epsilon decays over successive episodes, such that the policy gradually switches from random exploration to choosing the paths that maximize reward.\n","\n","Control the rate of epsilon's decay by multiplying epsilon by a decay factor on every episode. Set the decay factor between 0 and 1; typically very close to 1, such as 0.99. Therefore, the higher the decay factor, the slower epsilon decays. Understand this relation by running the following cell to visualize epsilon's decay. Try changing `episodes` and `eps_decay`, and check the result."]},{"cell_type":"code","metadata":{"id":"4YmCETqyp3Ng","colab_type":"code","colab":{}},"source":["eps_decay = 0.99\n","episodes = 100\n","\n","epsilon = 1.0\n","eps_values = np.zeros(episodes)\n","\n","for episode in range(episodes):\n","  eps_values[episode] = epsilon\n","  epsilon *= eps_decay\n","\n","# Plot epsilon values\n","plt.scatter(range(episodes),eps_values)\n","plt.xlabel('Episodes')\n","plt.ylabel('Epsilon')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziy0TJd46yTQ","colab_type":"text"},"source":["## Implement Epsilon Greedy Policy"]},{"cell_type":"markdown","metadata":{"id":"_G0PdQZp69NA","colab_type":"text"},"source":["In the following cell, implement the epsilon-greedy policy by selecting a random action or greedy action where indicated by `TODO`:"]},{"cell_type":"code","metadata":{"id":"A-nrruC7TxTc","colab_type":"code","colab":{}},"source":["def policy_eps_greedy(q_table, state, epsilon):\n","  if #TODO\n","    action = # TODO\n","  else:\n","    action = # TODO\n","  return action"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nsrRvEO7w2J","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n","def policy_eps_greedy(q_table, state, epsilon):\n","  if(np.random.random() < epsilon):\n","    action = policy_random()\n","  else:\n","    action = policy_greedy(q_table, state)\n","  return action"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G3S9xoyufsqH","colab_type":"text"},"source":["Using the function `policy_eps_greedy`, define a function to run an episode and update the Q-table:"]},{"cell_type":"code","metadata":{"id":"JJBp65zT68aX","colab_type":"code","colab":{}},"source":["def run_epsilon_greedy_episode(env, q_table, epsilon, learning_rate, discount_factor):\n","  state = env.reset()\n","  done = False\n","  episode_return = 0\n","  \n","  while(not done):\n","    action = policy_eps_greedy(q_table, state, epsilon)\n","    state_next, reward, done, _ = env.step(action)\n","    episode_return += reward\n","    q_table = bellman_update(q_table, learning_rate, discount_factor, reward, state, state_next, action)\n","    state = state_next\n","  \n","  return(reward, q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LHfvEC17l1R","colab_type":"text"},"source":["## Train Agent to Solve Frozen Lake"]},{"cell_type":"markdown","metadata":{"id":"r6_G4pT68dv7","colab_type":"text"},"source":["To train the agent, define a function that accepts the environment and agent hyperparameters, and runs episodes to update the Q-table."]},{"cell_type":"code","metadata":{"id":"NYKywMxN8uq7","colab_type":"code","colab":{}},"source":["def train_agent(env, epsiodes, learning_rate, discount_factor, eps_decay):\n","  reward_history = np.array([])\n","  q_table = np.zeros([num_states, num_actions])\n","  epsilon = 1.0\n","  \n","  for episode in range(episodes):\n","    reward, q_table = run_epsilon_greedy_episode(env, q_table, epsilon, learning_rate, discount_factor)\n","    reward_history = np.append(reward_history, reward)\n","    if(epsilon > EPS_MIN):\n","      epsilon *= eps_decay\n","  \n","  return(reward_history, q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jt03OHH4qdxG","colab_type":"text"},"source":["Train the agent to solve FrozenLake. The code displays the Q-table along with plots that show how success increases with episodes. Adjust hyperparameters to boost success.  For the solution, expand the following section.\n","\n","### How to Adjust Hyperparameters\n","\n","Adjusting hyperparameters in ML models is part art, part science. For general guidelines, see the [Testing & Debugging in Machine Learning](https://developers.google.com/machine-learning/testing-debugging/) course. In addition, consider the following guidelines for simple RL problems:\n","\n","* Results of RL training runs are variable because the environment is probabilistic, and because Q-values are arbitarily initialized. Do not rely on a single training run to evaluate a set of hyperparameter values. Try a few runs.\n","* If your agent is not successful, then perhaps epsilon decays before random exploration succeeds. Let your agent explore for longer by increasing `eps_decay`. Typically, `eps_decay` should be very close to 1.\n","* If your agent has a few successes, but does not exploit those successes, then try the following steps:\n","  * Adjust the learning rate. For general guidance on adjusting learning rate, see the section \"Adjust Hyperparameter Values\" on the page [Model Debugging](https://developers.google.com/machine-learning/testing-debugging/common/model-errors) from the Testing & Debugging course.\n","  * Train for longer by increasing the number of episodes.\n","  * Try increasing `discount_factor` to ensure that the reward backpropagates sufficiently to the Q-values for the initial states."]},{"cell_type":"code","metadata":{"id":"shvPve8POR-w","colab_type":"code","cellView":"form","colab":{}},"source":["# Hyperparameters\n","eps_decay = 0.9999 #@param\n","episodes = 5000    #@param\n","discount_factor = 0.8        #@param\n","learning_rate = 0.03       #@param\n","\n","# minimum value of epsilon is typically set to 0.01\n","EPS_MIN = 0.01\n","\n","# Run agent\n","reward_history, q_table = train_agent(env, episodes, learning_rate, discount_factor, eps_decay)\n","\n","# Normalize Q-values for readability\n","print(normalize_q_values(q_table))\n","\n","# Check success %\n","def check_success(env, q_table):\n","  success = 0\n","  for episode in range(100):\n","    state = env.reset()\n","    done = False\n","    reward = 0\n","    while not done:\n","      state, reward, done, _ = env.step(policy_greedy(q_table, state))\n","    success += reward\n","  print(\"\\nSuccess rate: \" + str(success) + \"%.\")\n","\n","check_success(env, q_table)\n","    \n","# Plot reward and success % over episodes.\n","def visualize_training(reward_history):\n","  plt.subplot(2,1,1)\n","  plt.plot(range(len(reward_history)), reward_history)\n","  plt.xlabel('Episodes')\n","  plt.ylabel('Reward')\n","  plt.title('Reward during Training')\n","  num_bins = episodes/100\n","  plt.subplot(2,1,2)\n","  plt.hist(np.nonzero(reward_history)[0], bins=int(num_bins), range=(0,episodes), rwidth=0.1)\n","  plt.xlabel('Episodes')\n","  plt.ylabel('% Success')\n","\n","visualize_training(reward_history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ag5jg3gVW5kT","colab_type":"text"},"source":["### Solution (expand to view)"]},{"cell_type":"markdown","metadata":{"id":"HdAZbF0GxPzw","colab_type":"text"},"source":["Run the following cell to successfully train the agent. Observe the following:\n","\n","* The epsilon-greedy policy calculates different Q-values from the purely random policy.\n","* The agent's success rate gradually increases as the agent learns the environment."]},{"cell_type":"code","metadata":{"id":"HRP2_da9W739","colab_type":"code","colab":{}},"source":["# Set parameters\n","eps_decay = 0.999\n","episodes = 5000\n","discount_factor = 0.95\n","learning_rate = 0.5\n","\n","# minimum value of epsilon is typically set to 0.01\n","EPS_MIN = 0.01\n","\n","# Run agent, print q-values, and plot reward history\n","reward_history, q_table = train_agent(env, episodes, learning_rate, discount_factor, eps_decay)\n","print(q_table)\n","visualize_training(reward_history)\n","check_success(env, q_table)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_iFIHw_sTiA","colab_type":"text"},"source":["## Conclusion and Next Steps"]},{"cell_type":"markdown","metadata":{"id":"Cq-ThveVsUvn","colab_type":"text"},"source":["The e-greedy policy successfully balances exploration and exploitation. Combining tabular Q-learning with an epsilon-greedy policy is a powerful approach to solving simple environments.\n","\n","Move onto the next Colab: [Deep Q-Learning](https://colab.research.google.com/drive/1XnFxIE882ptpO83mcAz7Zg8PxijJOsUs#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-deep-q-learning).\n","\n","For reference, the sequence of course Colabs is as follows:\n","\n","1. [Problem Framing in Reinforcement Learning](https://colab.research.google.com/drive/1sUYro4ZyiHuuKfy6KXFSdWjNlb98ZROd#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-problem-framing)\n","1. [Q-learning Framework](https://colab.research.google.com/drive/1ZPsEEu30SH1BUqUSxNsz0xeXL2Aalqfa#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-q-learning)\n","1. [Tabular Q-Learning](https://colab.research.google.com/drive/1sX2kO_RA1DckhCwX25OqjUVBATmOLgs2#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-tabular-q-learning)\n","1. [Deep Q-Learning](https://colab.research.google.com/drive/1XnFxIE882ptpO83mcAz7Zg8PxijJOsUs#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-deep-q-learning)\n","1. [Experience Replay and Target Networks](https://colab.research.google.com/drive/1DEv8FSjMvsgCDPlOGQrUFoJeAf67cFSo#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-experience-replay-and-target-networks)"]}]}